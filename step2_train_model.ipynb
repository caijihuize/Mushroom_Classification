{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Exon7Oj4z-g0",
        "outputId": "55b39730-2c6a-4a97-c8ec-ac5b67a4295f"
      },
      "outputs": [],
      "source": [
        "# 基础库\n",
        "import os                  # 提供与操作系统交互的功能，如文件路径操作\n",
        "import platform            # 获取系统平台信息\n",
        "import sys                 # 提供对Python解释器的访问和控制\n",
        "import urllib.request      # 用于处理URL和网络请求\n",
        "\n",
        "# 数据处理和科学计算\n",
        "import numpy as np         # 科学计算库，提供多维数组和矩阵运算\n",
        "import tensorflow as tf    # 深度学习框架，用于构建和训练神经网络\n",
        "\n",
        "# 数据可视化\n",
        "import matplotlib.pyplot as plt          # 绘图库，用于创建静态、动态和交互式可视化\n",
        "from matplotlib.font_manager import FontProperties, fontManager  # 字体管理，支持中文显示\n",
        "\n",
        "# Jupyter相关\n",
        "from IPython.display import clear_output  # 用于清除Jupyter notebook输出\n",
        "\n",
        "# Kaggle相关\n",
        "import kagglehub          # Kaggle数据集管理工具，用于下载和访问Kaggle数据集\n",
        "\n",
        "\n",
        "# 打印Python环境信息\n",
        "print(\"Python version:\", sys.version)     # 打印Python版本\n",
        "print(\"Platform:\", platform.platform())   # 打印操作系统平台（完整信息）\n",
        "print(\"System:\", platform.system())       # 打印操作系统类型（如Windows、Linux）\n",
        "print(\"Node:\", platform.node())           # 打印网络节点名称（主机名）\n",
        "print(\"Release:\", platform.release())     # 打印操作系统发行版本\n",
        "print(\"Version:\", platform.version())     # 打印操作系统版本号（更详细）\n",
        "print(\"Machine:\", platform.machine())     # 打印计算机类型（如x86_64）\n",
        "print(\"Processor:\", platform.processor()) # 打印处理器类型\n",
        "\n",
        "# 检查TensorFlow版本及其依赖\n",
        "!pip show tensorflow                      # 显示TensorFlow包的详细信息，包括版本和依赖\n",
        "\n",
        "# 检查系统硬件信息\n",
        "!cat /proc/cpuinfo | grep 'model name'    # 显示CPU型号（Linux系统）\n",
        "!cat /proc/meminfo | grep 'MemTotal'      # 显示系统总内存大小（Linux系统）\n",
        "!df -h                                    # 显示磁盘使用情况（disk free，人类可读格式）\n",
        "!nvidia-smi                               # 显示NVIDIA GPU信息（如果系统有NVIDIA GPU）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL7FdUwf7_M1"
      },
      "source": [
        "# 数据集\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJmst4Ck8UIJ"
      },
      "source": [
        "## 数据集获取\n",
        " \n",
        "本项目使用的蘑菇图像数据集托管在 [Kaggle](https://www.kaggle.com/) 平台上，数据集名称为 [huizecai/mushroom](https://www.kaggle.com/datasets/huizecai/mushroom)。该数据集包含了多种常见蘑菇的高清图片，以及对应的分类标签。\n",
        "\n",
        "为了方便数据获取，我们使用 `kagglehub` 库来自动下载和管理数据集。下面的代码单元格会直接从 Kaggle 下载数据集，并返回保存在本地的路径。数据集下载完成后会被缓存，后续运行时将直接使用缓存版本，无需重复下载。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE-AmgP8cAmb",
        "outputId": "4c30f603-73a3-4078-aacf-7ce08f7e1f63",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 设置数据集名称\n",
        "dataset_name = \"huizecai/mushroom\"  # 指定要下载的Kaggle数据集名称\n",
        "\n",
        "# 使用KaggleHub下载数据集\n",
        "path = kagglehub.dataset_download(dataset_name)  # 下载数据集并获取保存路径\n",
        "\n",
        "# 打印数据集文件的保存路径\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# 设置数据和标签文件的具体路径\n",
        "dataset_path = path + '/archive/data'  # 图片数据所在目录的路径\n",
        "label_path = path + '/archive/label.txt'  # 标签文件的路径"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVChVsT_97wJ"
      },
      "source": [
        "## 数据集类别统计分析\n",
        " \n",
        "为了避免TensorFlow处理中文路径时可能出现的编码问题，本数据集采用了规范化的命名方式:\n",
        " - 各蘑菇种类的文件夹以\"classXX\"格式命名(XX为数字编号)\n",
        " - 使用label.txt文件建立文件夹编号与中文名称的映射关系\n",
        " - 这种设计既保证了系统兼容性，又方便了数据的管理和使用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN4YxYi5cAmc",
        "outputId": "8cb66590-d88b-4b99-a0cb-863ff6fdebf9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 获取所有子目录（即蘑菇类别）\n",
        "# os.listdir() 列出指定目录下的所有文件和文件夹\n",
        "# os.path.isdir() 判断是否为文件夹\n",
        "# 使用列表推导式获取所有蘑菇类别的文件夹名\n",
        "dir_names = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "\n",
        "# 读取 label.txt 并解析内容\n",
        "# 创建一个空字典用于存储类别ID和名称的映射关系\n",
        "categories = {}\n",
        "# 以UTF-8编码打开label.txt文件\n",
        "with open(label_path, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        # 去除每行首尾空白字符并按空格分割\n",
        "        parts = line.strip().split()\n",
        "        # 确保每行包含两个部分:类别名称和ID\n",
        "        if len(parts) == 2:\n",
        "            category_name = parts[0]  # 第一部分为类别名称(中文)\n",
        "            category_id = parts[1]    # 第二部分为类别ID\n",
        "            categories[category_id] = category_name  # 建立ID到名称的映射\n",
        "\n",
        "# 统计每种类别的图像数量\n",
        "# 创建空字典存储每个类别的图片数量\n",
        "category_counts = {}\n",
        "for category_id in categories.keys():\n",
        "    # 确保目录存在再进行统计\n",
        "    if category_id in dir_names:\n",
        "        # 构建完整的类别目录路径\n",
        "        category_dir = os.path.join(dataset_path, category_id)\n",
        "        # 统计jpg和jpeg格式的图片数量\n",
        "        # 使用列表推导式过滤出图片文件并计数\n",
        "        num_images = len([f for f in os.listdir(category_dir) if f.endswith('.jpg') or f.endswith('.jpeg')])\n",
        "        # 使用中文类别名称作为键存储图片数量\n",
        "        category_counts[categories[category_id]] = num_images\n",
        "\n",
        "# 打印每个类别的图片数量统计结果\n",
        "print(\"Category counts:\", category_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i374m8Jd-ob6"
      },
      "source": [
        "## 解决matplotlib中文显示问题\n",
        "\n",
        "matplotlib默认不支持中文字体显示,可能会出现乱码。为了确保数据可视化结果能正确展示中文:\n",
        "1. 我们将下载并使用\"SimHei\"(黑体)字体\n",
        "2. 注册字体到matplotlib的字体管理器\n",
        "3. 配置全局字体设置\n",
        "\n",
        "这样可以保证后续所有图表中的中文标题、标签等都能正常显示。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "Hv_Ltg1xcAmd",
        "outputId": "4e8a29c4-5b7b-4604-9f7c-ed8812bf0e6e"
      },
      "outputs": [],
      "source": [
        "# 创建img目录(如果不存在)\n",
        "if not os.path.exists('img'):\n",
        "    os.makedirs('img')\n",
        "\n",
        "# 设置字体文件的URL和本地保存路径\n",
        "font_url = \"https://github.com/caijihuize/Mushroom_Classification/raw/main/SimHei.ttf\"  # 黑体字体文件的URL\n",
        "font_name = \"SimHei.ttf\"  # 本地保存的字体文件名\n",
        "\n",
        "# 如果字体文件不存在则下载\n",
        "if not os.path.exists(font_name):\n",
        "    urllib.request.urlretrieve(font_url, font_name)  # 下载字体文件到本地\n",
        "\n",
        "# 配置matplotlib的字体设置\n",
        "fontManager.addfont(font_name)  # 将字体文件添加到matplotlib的字体管理器\n",
        "font_prop = FontProperties(fname=font_name)  # 创建字体属性对象\n",
        "\n",
        "# 设置全局字体配置\n",
        "plt.rcParams['font.family'] = 'SimHei'  # 设置默认字体为黑体\n",
        "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
        "plt.rcParams['font.size'] = 20  # 设置全局字体大小\n",
        "plt.rcParams['axes.titlesize'] = 22  # 设置标题字体大小\n",
        "plt.rcParams['axes.labelsize'] = 20  # 设置轴标签字体大小\n",
        "plt.rcParams['figure.dpi'] = 300  # 设置图形DPI为300,提高显示清晰度\n",
        "plt.rcParams['savefig.dpi'] = 600  # 设置保存图片的DPI为600,提高保存图片的清晰度\n",
        "\n",
        "# 绘制测试图表验证中文显示\n",
        "plt.figure(figsize=(10, 6))  # 设置图形大小\n",
        "plt.title('这是一个标题', fontsize=20)  # 设置标题\n",
        "plt.xlabel('X轴标签', fontsize=16)  # 设置X轴标签\n",
        "plt.ylabel('Y轴标签', fontsize=16)  # 设置Y轴标签\n",
        "plt.plot([0, 1, 2, 3], [0, 1, 4, 9], linewidth=2)  # 绘制简单的折线图,增加线宽提高清晰度\n",
        "\n",
        "# 保存图表到img目录,使用更高质量的设置\n",
        "plt.savefig('img/test_plot.png', \n",
        "            bbox_inches='tight',  # 自动调整边界\n",
        "            dpi=800,  # 设置更高的DPI\n",
        "            format='png',  # 使用PNG格式保存\n",
        "            facecolor='white',  # 设置白色背景\n",
        "            edgecolor='none',  # 无边框\n",
        "            transparent=False)  # 不透明\n",
        "plt.show()  # 显示图表"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-jZkqsW_tQ6"
      },
      "source": [
        "## 绘制各种类图片数量的柱状图"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "9aAf366McAmd",
        "outputId": "5fa50055-8c01-48be-c3f3-99f06df8c517",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 准备数据\n",
        "categories_readable = list(category_counts.keys())  # 获取所有蘑菇种类名称\n",
        "counts = list(category_counts.values())  # 获取每个种类对应的图片数量\n",
        "\n",
        "# 创建一个新的图形，设置更大的尺寸以便更好地展示数据\n",
        "plt.figure(figsize=(18, 16))\n",
        "\n",
        "# 创建颜色渐变 - 根据数量排序，数量越多颜色越亮\n",
        "# 首先创建数量和类别的映射关系\n",
        "count_category_pairs = list(zip(counts, categories_readable))\n",
        "# 按照数量排序\n",
        "count_category_pairs.sort(key=lambda x: x[0])\n",
        "# 提取排序后的类别和数量\n",
        "sorted_categories = [pair[1] for pair in count_category_pairs]\n",
        "sorted_counts = [pair[0] for pair in count_category_pairs]\n",
        "\n",
        "# 创建颜色映射，使用较为柔和的颜色差异\n",
        "# 使用单一色系的渐变，减小颜色差异\n",
        "norm = plt.Normalize(min(counts), max(counts))\n",
        "# 使用Blues色系，颜色差异较小\n",
        "colors = plt.cm.Blues(norm(counts) * 0.7 + 0.3)  # 缩小颜色范围，增加最小值，减小颜色差异\n",
        "\n",
        "# 绘制水平柱状图，使用柔和的颜色\n",
        "bars = plt.barh(categories_readable, counts, color=colors, height=0.7, \n",
        "                edgecolor='gray', linewidth=0.5, alpha=0.9)  # 增加透明度使颜色更柔和\n",
        "\n",
        "# 在每个柱子右侧添加数值标签\n",
        "for i, bar in enumerate(bars):\n",
        "    width = bar.get_width()  # 获取柱子的宽度(即图片数量)\n",
        "    plt.text(width + 2, bar.get_y() + bar.get_height()/2, f'{int(width)}', \n",
        "             va='center', ha='left', fontsize=14, fontweight='bold', \n",
        "             color='darkblue')  # 美化标签样式\n",
        "\n",
        "# 设置图表标题和轴标签\n",
        "plt.title('各蘑菇种类的图片数量分布', fontsize=28, fontweight='bold', pad=20)  # 增大标题字体并加粗\n",
        "plt.xlabel('图片数量', fontsize=22, labelpad=15)  # 设置x轴标签并增加内边距\n",
        "plt.ylabel('蘑菇种类', fontsize=22, labelpad=15)  # 设置y轴标签并增加内边距\n",
        "\n",
        "# 设置坐标轴样式\n",
        "plt.tick_params(axis='both', which='major', labelsize=16)  # 增大刻度标签字体\n",
        "plt.xlim(25, max(counts) + max(counts)*0.1)  # 设置x轴范围，留出更多空间\n",
        "\n",
        "# 添加网格线\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.4, color='gray')  # 使用更淡的网格线\n",
        "\n",
        "# 添加背景色\n",
        "plt.gca().set_facecolor('#f8f9fa')  # 设置浅灰色背景\n",
        "plt.gca().spines['top'].set_visible(False)  # 移除上边框\n",
        "plt.gca().spines['right'].set_visible(False)  # 移除右边框\n",
        "plt.gca().spines['left'].set_linewidth(0.5)  # 减小左边框宽度\n",
        "plt.gca().spines['bottom'].set_linewidth(0.5)  # 减小下边框宽度\n",
        "\n",
        "# 自动调整布局，防止标签被截断\n",
        "plt.tight_layout()\n",
        "\n",
        "# 保存图表到img目录，使用更高质量的设置\n",
        "plt.savefig('img/mushroom_distribution.png', \n",
        "            bbox_inches='tight', \n",
        "            dpi=800, \n",
        "            facecolor='#f8f9fa')  # 保存高质量图片，保持背景色一致\n",
        "\n",
        "# 显示图形\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP2oyCdcDePo"
      },
      "source": [
        "# 训练准备工作"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IInl-UREEOOr"
      },
      "source": [
        "## 加载图像数据集\n",
        "\n",
        "使用 TensorFlow 的 [image_dataset_from_directory](https://tensorflow.google.cn/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory?hl=en) 函数加载和准备图像数据集：\n",
        "\n",
        "\n",
        "*   directory=dataset_path ：指定图像数据所在的路径。\n",
        "*   image_size=(224, 224) ：指定每个图像的大小为224x224像素。\n",
        "*   batch_size=32 ：指定每个批次包含32张图像。\n",
        "*   validation_split=0.1 ：指定10%的数据作为验证集。\n",
        "*   subset='both' ：指定同时返回训练集和验证集。\n",
        "*   label_mode='categorical' ：指定标签模式为分类模式，返回one-hot编码的标签。\n",
        "*   seed=21 ：设置随机种子以确保数据集的可重复性。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jxk-SQQ-fiNv",
        "outputId": "e403800a-0a79-4bdd-d584-f51905c37f79"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 加载和准备图像数据集\n",
        "train_dataset, validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=dataset_path,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    subset='both',\n",
        "    label_mode='categorical',\n",
        "    seed=44\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iVqn2bVNewl"
      },
      "source": [
        "## 计算训练集和验证集中各类别图像的分布情况\n",
        "\n",
        "下面我们将统计训练集和验证集中每个蘑菇类别的图像数量，以便了解数据集的分布特征。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0a6DQ-dIQSN",
        "outputId": "510f098d-7cd7-4fc3-bc7f-cc5df74dfb91"
      },
      "outputs": [],
      "source": [
        "# 获取类别名称\n",
        "class_names = train_dataset.class_names\n",
        "# 从训练数据集中获取所有类别的名称列表\n",
        "\n",
        "# 初始化字典用于存储每种类别的图像数量\n",
        "train_category_counts = {name: 0 for name in categories.values()}\n",
        "validation_category_counts = {name: 0 for name in categories.values()}\n",
        "# 创建两个字典，分别用于存储训练集和验证集中每个类别的图像数量\n",
        "# 使用字典推导式初始化，键为类别的中文名称，值初始化为0\n",
        "\n",
        "# 统计训练集中的图像数量\n",
        "for images, labels in train_dataset:\n",
        "    # 遍历训练数据集中的每个批次，每个批次包含图像和对应的标签\n",
        "    for label in labels.numpy():\n",
        "        # 将标签张量转换为numpy数组并遍历\n",
        "        category_name = class_names[np.argmax(label)]\n",
        "        # np.argmax(label)找出one-hot编码中值为1的索引位置\n",
        "        # 通过索引从class_names中获取对应的类别名称\n",
        "        train_category_counts[categories[category_name]] += 1\n",
        "        # 将该类别在训练集中的计数加1，使用categories字典将英文名映射为中文名\n",
        "\n",
        "# 统计验证集中的图像数量\n",
        "for images, labels in validation_dataset:\n",
        "    # 遍历验证数据集中的每个批次\n",
        "    for label in labels.numpy():\n",
        "        # 同样处理验证集中的标签\n",
        "        category_name = class_names[np.argmax(label)]\n",
        "        validation_category_counts[categories[category_name]] += 1\n",
        "        # 将该类别在验证集中的计数加1\n",
        "\n",
        "# 打印统计结果\n",
        "print(\"训练集类别图像数量统计:\", train_category_counts)\n",
        "print(\"验证集类别图像数量统计:\", validation_category_counts)\n",
        "# 输出训练集和验证集中各个类别的图像数量统计结果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9PmfburNand"
      },
      "source": [
        "## 绘制训练集和验证集的柱状图"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "VR2URGk4JLd8",
        "outputId": "d10c2735-93cc-493a-fe3f-8ac5446ee1b8"
      },
      "outputs": [],
      "source": [
        "# 准备数据\n",
        "categories_readable = list(categories.values())  # 使用中文名映射\n",
        "train_counts = [train_category_counts[name] for name in categories_readable]\n",
        "validation_counts = [validation_category_counts[name] for name in categories_readable]\n",
        "\n",
        "# 创建一个新的图形\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# 设置柱子的位置\n",
        "bar_width = 0.35\n",
        "index = np.arange(len(categories_readable))\n",
        "\n",
        "# 绘制训练集柱状图\n",
        "bars_train = plt.barh(index + bar_width, train_counts, bar_width, label='训练集', color='skyblue')\n",
        "\n",
        "# 绘制验证集柱状图\n",
        "bars_validation = plt.barh(index, validation_counts, bar_width, label='测试集', color='orange')\n",
        "\n",
        "# 添加数据标签\n",
        "def add_labels(bars):\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()\n",
        "        plt.text(width + 1, bar.get_y() + bar.get_height() / 2, '%d' % int(width), ha='left', va='center')\n",
        "\n",
        "add_labels(bars_train)\n",
        "add_labels(bars_validation)\n",
        "\n",
        "# 设置标题和标签\n",
        "plt.title('各蘑菇种类的图片数量 (Train vs Validation)', fontsize=20)\n",
        "plt.xlabel('图片数量', fontsize=16)\n",
        "plt.ylabel('蘑菇种类', fontsize=16)\n",
        "\n",
        "# 设置Y轴刻度\n",
        "plt.yticks(index + bar_width / 2, categories_readable)\n",
        "\n",
        "# 显示网格\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "# 添加图例\n",
        "plt.legend()\n",
        "\n",
        "# 自动调整布局\n",
        "plt.tight_layout()\n",
        "\n",
        "# 显示图形\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3psFiWZHOXw"
      },
      "source": [
        "## 显示数据集中的图像样本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "9lZeeEEgesTm",
        "outputId": "b9dfcb59-9b83-4372-fa27-df778c7e070f"
      },
      "outputs": [],
      "source": [
        "# 获取类别名称\n",
        "class_names = train_dataset.class_names\n",
        "\n",
        "# 设置要显示的图像数量\n",
        "num_images_to_show = 4\n",
        "images_to_display = []\n",
        "labels_to_display = []\n",
        "\n",
        "# 从数据集中获取随机选择的图像和标签\n",
        "for images, labels in train_dataset.take(1):\n",
        "    indices = np.random.choice(range(images.shape[0]), num_images_to_show, replace=False)\n",
        "    for index in indices:\n",
        "        images_to_display.append(images[index])\n",
        "        labels_to_display.append(labels[index])\n",
        "\n",
        "# 创建图形并显示图像\n",
        "fig, axes = plt.subplots(1, num_images_to_show, figsize=(12, 4))\n",
        "for i, (image, label) in enumerate(zip(images_to_display, labels_to_display)):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(image.numpy().astype(\"uint8\"))\n",
        "    ax.set_title(categories[class_names[np.argmax(label.numpy())]], fontsize=20)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3cwpuE9O5nP"
      },
      "source": [
        "# 数据增强"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIJTyyTQP1fL"
      },
      "source": [
        "数据增强是一种常用的技术，用于通过变换现有数据来增加训练数据的多样性，从而提高模型的泛化能力。在图像处理中，常见的数据增强技术包括旋转、翻转、缩放、裁剪、亮度调整等。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW2ujx4-RvQg"
      },
      "source": [
        "## 定义数据增强层"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "n1bKrejhgePO"
      },
      "outputs": [],
      "source": [
        "# 定义数据增强的预处理层\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),  # 水平翻转\n",
        "    tf.keras.layers.RandomRotation(0.2),       # 随机旋转最多20%\n",
        "    tf.keras.layers.RandomZoom(0.2, 0.2),      # 随机缩放\n",
        "    tf.keras.layers.RandomContrast(0.2),       # 随机对比度调整\n",
        "    tf.keras.layers.RandomBrightness(0.2)      # 随机亮度调整\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I53w8VKuRz9t"
      },
      "source": [
        "## 定义显示数据增强示例函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-FayHXVxtypj"
      },
      "outputs": [],
      "source": [
        "def demo_augmentation(sample_image, model, num_aug):\n",
        "    '''Takes a single image array, then uses a model to generate num_aug transformations'''\n",
        "\n",
        "    # Instantiate preview list\n",
        "    image_preview = []\n",
        "\n",
        "    # Convert input image to a PIL image instance\n",
        "    sample_image_pil = tf.keras.utils.array_to_img(sample_image)\n",
        "\n",
        "    # Append the result to the list\n",
        "    image_preview.append(sample_image_pil)\n",
        "\n",
        "    # Apply the image augmentation and append the results to the list\n",
        "    for i in range(NUM_AUG):\n",
        "        sample_image_aug = model(tf.expand_dims(sample_image, axis=0))\n",
        "        sample_image_aug_pil = tf.keras.utils.array_to_img(tf.squeeze(sample_image_aug))\n",
        "        image_preview.append(sample_image_aug_pil)\n",
        "\n",
        "    # Instantiate a subplot\n",
        "    fig, axes = plt.subplots(1, NUM_AUG + 1, figsize=(12, 12))\n",
        "\n",
        "    # Preview the images.\n",
        "    for index, ax in enumerate(axes):\n",
        "        ax.imshow(image_preview[index])\n",
        "        ax.set_axis_off()\n",
        "\n",
        "        if index == 0:\n",
        "            ax.set_title('original')\n",
        "        else:\n",
        "            ax.set_title(f'augment {index}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvJSBnm7SK5c"
      },
      "source": [
        "## 获取随机批次"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55HfA-g5uG8o",
        "outputId": "698265b2-710b-46ce-ad38-401e890c1554"
      },
      "outputs": [],
      "source": [
        "# 获取训练集中随机的一批次图片\n",
        "sample_batch = list(train_dataset.take(1))[0][0]\n",
        "print(f'images per batch: {len(sample_batch)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7YzAYs5VBUq"
      },
      "source": [
        "## 显示数据增强示例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "mfeF4w3Qt725",
        "outputId": "0c03a95c-8411-4c6e-994f-69efda80073f"
      },
      "outputs": [],
      "source": [
        "NUM_AUG = 5\n",
        "\n",
        "# Apply the transformations to the first 4 images\n",
        "demo_augmentation(sample_batch[0], data_augmentation, NUM_AUG)\n",
        "demo_augmentation(sample_batch[1], data_augmentation, NUM_AUG)\n",
        "demo_augmentation(sample_batch[2], data_augmentation, NUM_AUG)\n",
        "demo_augmentation(sample_batch[3], data_augmentation, NUM_AUG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMccJ_imamqA"
      },
      "source": [
        "# 常用显示函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "I9GNzqANFicv"
      },
      "outputs": [],
      "source": [
        "# 训练模型\n",
        "def train_model(model, train_dataset=train_dataset, validation_dataset=validation_dataset, epochs=30):\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)\n",
        "\n",
        "    history = model.fit(train_dataset, epochs=epochs, validation_data=validation_dataset, callbacks=[early_stopping, lr_scheduler])\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hcr0xToSWwun"
      },
      "outputs": [],
      "source": [
        "# 量化与格式转换\n",
        "def quantize_and_convert_to_tflite(model):\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "    tflite_quant_model = converter.convert()\n",
        "\n",
        "    return tflite_quant_model\n",
        "\n",
        "def save_model(model, output_path):\n",
        "  with open(output_path, 'wb') as f:\n",
        "    f.write(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "magj2cqVUiZJ"
      },
      "outputs": [],
      "source": [
        "def show_history(history):\n",
        "  # 绘制训练过程中的损失和准确率曲线\n",
        "  plt.figure(figsize=(12, 4))\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(history.history['loss'], label='Training Loss')\n",
        "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "  plt.title('Loss Over Epochs')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "  plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "  plt.title('Accuracy Over Epochs')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dVpHWE1qwPKp"
      },
      "outputs": [],
      "source": [
        "def combine_history(histories, labels):\n",
        "  min_epochs = min([len(history.history['loss']) for history in histories])\n",
        "  epochs = range(1, min_epochs + 1)\n",
        "\n",
        "  plt.figure(figsize=(14, 6))\n",
        "\n",
        "  # (1) 绘制损失曲线\n",
        "  plt.subplot(1, 2, 1)\n",
        "  for i, history in enumerate(histories):\n",
        "      train_loss = history.history['loss'][:min_epochs]\n",
        "      val_loss = history.history['val_loss'][:min_epochs]\n",
        "      plt.plot(epochs, train_loss, label=f'{labels[i]} Train Loss', linestyle='--')\n",
        "      plt.plot(epochs, val_loss, label=f'{labels[i]} Val Loss')\n",
        "\n",
        "  plt.title('Training and Validation Loss Comparison')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "  # (2) 绘制准确率曲线（如果需要）\n",
        "  plt.subplot(1, 2, 2)\n",
        "  for i, history in enumerate(histories):\n",
        "      train_acc = history.history['accuracy'][:min_epochs]\n",
        "      val_acc = history.history['val_accuracy'][:min_epochs]\n",
        "      plt.plot(epochs, train_acc, label=f'{labels[i]} Train Accuracy', linestyle='--')\n",
        "      plt.plot(epochs, val_acc, label=f'{labels[i]} Val Accuracy')\n",
        "\n",
        "  plt.title('Training and Validation Accuracy Comparison')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "  # 显示图表\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "geSO-2Iyn777"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# 在训练完成后计算每个类别的准确率\n",
        "def calculate_class_accuracies(model, dataset):\n",
        "    all_predictions = []\n",
        "    all_true_labels = []\n",
        "\n",
        "    for images, labels in dataset:\n",
        "        predictions = model.predict(images)\n",
        "        predicted_classes = np.argmax(predictions, axis=1)\n",
        "        true_classes = np.argmax(labels, axis=1)\n",
        "\n",
        "        all_predictions.extend(predicted_classes)\n",
        "        all_true_labels.extend(true_classes)\n",
        "\n",
        "    class_accuracies = {}\n",
        "    for class_index in range(len(class_names)):\n",
        "        mask = (np.array(all_true_labels) == class_index)\n",
        "        correct_predictions = np.sum(np.array(all_predictions)[mask] == class_index)\n",
        "        total_samples = np.sum(mask)\n",
        "        if total_samples > 0:\n",
        "            accuracy = correct_predictions / total_samples\n",
        "        else:\n",
        "            accuracy = 0\n",
        "        class_accuracies[class_names[class_index]] = accuracy\n",
        "\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # 准备数据\n",
        "    categories_readable = list(categories[class_name] for class_name in class_accuracies.keys())\n",
        "    accuracies = list(class_accuracies.values())\n",
        "\n",
        "    # 创建一个新的图形\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    # 绘制柱状图\n",
        "    bars = plt.barh(categories_readable, accuracies, color='skyblue')\n",
        "\n",
        "    # 添加数据标签\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()\n",
        "        plt.text(width + 0.01, bar.get_y() + bar.get_height()/2, f'{width:.4f}', va='center')\n",
        "\n",
        "    # 设置标题和标签\n",
        "    plt.title('各类别准确率', fontsize=20)\n",
        "    plt.xlabel('准确率', fontsize=16)\n",
        "    plt.ylabel('类别', fontsize=16)\n",
        "\n",
        "    # 设置网格\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "    # 自动调整布局\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # 显示图形\n",
        "    plt.show()\n",
        "\n",
        "    return class_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1YgDWOLmDn3J"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataset):\n",
        "        \"\"\"使用TensorFlow内置metrics评估Keras模型在数据集上的性能\"\"\"\n",
        "        # 计算模型大小 (MB)\n",
        "        model_size_mb = sum(np.prod(w.shape) for w in model.get_weights()) * 4 / (1024 * 1024)  # 假设浮点数为4字节\n",
        "\n",
        "        # 创建评估指标\n",
        "        top1_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "        top5_accuracy = tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_accuracy')\n",
        "\n",
        "        # 在数据集上运行评估\n",
        "        loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "        loss_avg = tf.keras.metrics.Mean()\n",
        "\n",
        "        for images, labels in dataset:\n",
        "            predictions = model(images, training=False)\n",
        "            loss = loss_object(labels, predictions)\n",
        "\n",
        "            loss_avg.update_state(loss)\n",
        "            top1_accuracy.update_state(labels, predictions)\n",
        "            top5_accuracy.update_state(labels, predictions)\n",
        "\n",
        "        # 获取结果\n",
        "        loss = loss_avg.result().numpy()\n",
        "        top1 = top1_accuracy.result().numpy()\n",
        "        top5 = top5_accuracy.result().numpy()\n",
        "\n",
        "        print(f\"Loss: {loss:.4f}\")\n",
        "        print(f\"Top-1 Accuracy: {top1:.4f}\")\n",
        "        print(f\"Top-5 Accuracy: {top5:.4f}\")\n",
        "        print(f\"Model Size: {model_size_mb:.2f} MB\")\n",
        "\n",
        "        return top1, top5, model_size_mb\n",
        "\n",
        "def evaluate_tflite_model(tflite_model, dataset):\n",
        "    \"\"\"使用TensorFlow Lite工具简化的评估函数\"\"\"\n",
        "    # 计算TFLite模型大小 (MB)\n",
        "    model_size_mb = len(tflite_model) / (1024 * 1024)\n",
        "\n",
        "    # 创建TFLite解释器\n",
        "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # 获取输入输出细节\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    # 创建评估指标\n",
        "    top1_accuracy = tf.keras.metrics.Accuracy()\n",
        "    top5_accuracy = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)\n",
        "\n",
        "    # 评估进度计数器\n",
        "    total_samples = 0\n",
        "\n",
        "    # 遍历数据集\n",
        "    for images, labels in dataset:\n",
        "        # 获取标签的真实类别（稀疏形式）\n",
        "        true_labels = tf.argmax(labels, axis=1).numpy()\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "        batch_predictions = np.zeros((batch_size, len(class_names)), dtype=np.float32)\n",
        "\n",
        "        # 逐个样本进行推理\n",
        "        for i in range(batch_size):\n",
        "            total_samples += 1\n",
        "\n",
        "            # 预处理输入\n",
        "            input_data = np.expand_dims(images[i].numpy(), axis=0).astype(np.float32)\n",
        "\n",
        "            # 设置输入张量\n",
        "            interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "            # 运行推理\n",
        "            interpreter.invoke()\n",
        "\n",
        "            # 获取输出并保存\n",
        "            batch_predictions[i] = interpreter.get_tensor(output_details[0]['index'])[0]\n",
        "\n",
        "        # 更新指标（整个批次一次更新）\n",
        "        top1_accuracy.update_state(\n",
        "            true_labels,\n",
        "            np.argmax(batch_predictions, axis=1)\n",
        "        )\n",
        "        top5_accuracy.update_state(\n",
        "            true_labels,\n",
        "            batch_predictions\n",
        "        )\n",
        "\n",
        "    # 获取结果\n",
        "    top1 = top1_accuracy.result().numpy()\n",
        "    top5 = top5_accuracy.result().numpy()\n",
        "\n",
        "    print(f\"TFLite模型 Top-1 准确率: {top1:.4f}\")\n",
        "    print(f\"TFLite模型 Top-5 准确率: {top5:.4f}\")\n",
        "    print(f\"TFLite模型大小: {model_size_mb:.2f} MB\")\n",
        "\n",
        "    return top1, top5, model_size_mb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWZyIg8ET-S2"
      },
      "source": [
        "# MobileNetV1模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VwMV06p-UBQC"
      },
      "outputs": [],
      "source": [
        "def get_MobileNetV1():\n",
        "  # 初始化基础模型\n",
        "  pre_trained_model = tf.keras.applications.MobileNet(\n",
        "      input_shape=(224, 224, 3),\n",
        "      include_top=False,\n",
        "      weights='imagenet'\n",
        "  )\n",
        "\n",
        "  # 冻结基础模型的权重\n",
        "  for layer in pre_trained_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  # 应用数据增强和预处理\n",
        "  x = data_augmentation(pre_trained_model.input)\n",
        "  x = tf.keras.applications.mobilenet.preprocess_input(x)\n",
        "\n",
        "  # 添加自定义顶层分类器\n",
        "  x = pre_trained_model(x)\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)\n",
        "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "  predictions = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)\n",
        "\n",
        "  # 构建最终模型\n",
        "  MobileNetV1 = tf.keras.models.Model(inputs=pre_trained_model.input, outputs=predictions)\n",
        "\n",
        "  return MobileNetV1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVOJo9-FbdpX"
      },
      "source": [
        "# MobileNetV2模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jUdyuZwGbiuk"
      },
      "outputs": [],
      "source": [
        "def get_MobileNetV2():\n",
        "  # 初始化基础模型\n",
        "  pre_trained_model = tf.keras.applications.MobileNetV2(\n",
        "      input_shape=(224, 224, 3),\n",
        "      include_top=False,\n",
        "      weights='imagenet'\n",
        "  )\n",
        "\n",
        "  # 冻结基础模型的权重\n",
        "  for layer in pre_trained_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  # 应用数据增强和预处理\n",
        "  x = data_augmentation(pre_trained_model.input)\n",
        "  x = tf.keras.applications.mobilenet_v2.preprocess_input(x)\n",
        "\n",
        "  # 添加自定义顶层分类器\n",
        "  x = pre_trained_model(x)\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)\n",
        "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "  predictions = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)\n",
        "\n",
        "  # 构建最终模型\n",
        "  MobileNetV2 = tf.keras.models.Model(inputs=pre_trained_model.input, outputs=predictions)\n",
        "\n",
        "  return MobileNetV2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "savp0b1IWtC2"
      },
      "source": [
        "# EfficientNet模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "eHvAadCCWyUl"
      },
      "outputs": [],
      "source": [
        "def get_EfficientNetB0():\n",
        "  # 初始化基础模型\n",
        "  pre_trained_model = tf.keras.applications.EfficientNetB0(\n",
        "      input_shape=(224, 224, 3),\n",
        "      include_top=False,\n",
        "      weights='imagenet'\n",
        "  )\n",
        "\n",
        "  # 冻结基础模型的权重\n",
        "  for layer in pre_trained_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  # 应用数据增强和预处理\n",
        "  x = data_augmentation(pre_trained_model.input)\n",
        "  x = tf.keras.applications.efficientnet.preprocess_input(x)\n",
        "\n",
        "  # 添加自定义顶层分类器\n",
        "  x = pre_trained_model(x)\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)\n",
        "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "  predictions = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)\n",
        "\n",
        "  # 构建最终模型\n",
        "  EfficientNetB0 = tf.keras.models.Model(inputs=pre_trained_model.input, outputs=predictions)\n",
        "\n",
        "  return EfficientNetB0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fvu3Ux7k-xq"
      },
      "source": [
        "# ResNet模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CI6EnoEZlG7p"
      },
      "outputs": [],
      "source": [
        "def get_ResNet101():\n",
        "  # 初始化基础模型\n",
        "  pre_trained_model = tf.keras.applications.ResNet101(\n",
        "      input_shape=(224, 224, 3),\n",
        "      include_top=False,\n",
        "      weights='imagenet'\n",
        "  )\n",
        "\n",
        "  # 冻结基础模型的权重\n",
        "  for layer in pre_trained_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  # 应用数据增强和预处理\n",
        "  x = data_augmentation(pre_trained_model.input)\n",
        "  x = tf.keras.applications.resnet.preprocess_input(x)\n",
        "\n",
        "  # 添加自定义顶层分类器\n",
        "  x = pre_trained_model(x)\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)\n",
        "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "  predictions = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)\n",
        "\n",
        "  # 构建最终模型\n",
        "  ResNet101 = tf.keras.models.Model(inputs=pre_trained_model.input, outputs=predictions)\n",
        "\n",
        "  return ResNet101"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlXKKN2qF1jT"
      },
      "source": [
        "# 训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "tGcDi25UF47l",
        "outputId": "d35026eb-6171-47b6-bf33-62d2157c7348"
      },
      "outputs": [],
      "source": [
        "MobileNetV1 = get_MobileNetV1()\n",
        "show_history(train_model(MobileNetV1))\n",
        "MobileNetV1_tflite = quantize_and_convert_to_tflite(MobileNetV1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "_vy9vSGUOacN",
        "outputId": "67794e61-df96-4fd2-856d-0158091b2d15"
      },
      "outputs": [],
      "source": [
        "MobileNetV2 = get_MobileNetV2()\n",
        "show_history(train_model(MobileNetV2))\n",
        "MobileNetV2_tflite = quantize_and_convert_to_tflite(MobileNetV2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "E9GZPspnuU0z",
        "outputId": "7be235aa-61e8-47bf-803e-f70fae4909dd"
      },
      "outputs": [],
      "source": [
        "EfficientNetB0 = get_EfficientNetB0()\n",
        "show_history(train_model(EfficientNetB0))\n",
        "EfficientNetB0_tflite = quantize_and_convert_to_tflite(EfficientNetB0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "bZopLrSXOcQ7",
        "outputId": "4253acaf-477b-4e91-e58a-9195d25271ff"
      },
      "outputs": [],
      "source": [
        "ResNet101 = get_ResNet101()\n",
        "show_history(train_model(ResNet101))\n",
        "ResNet101_tflite = quantize_and_convert_to_tflite(ResNet101)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbUitwGkJ07U"
      },
      "source": [
        "# 评估"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV1PV-blGVlT",
        "outputId": "a78a3773-4ab8-4597-af8c-117daea41672"
      },
      "outputs": [],
      "source": [
        "print(\"MobileNetV1:\\n\")\n",
        "evaluate_model(MobileNetV1, validation_dataset)\n",
        "evaluate_tflite_model(MobileNetV1_tflite, validation_dataset)\n",
        "save_model(MobileNetV1_tflite, \"MobileNetV1.tflite\")\n",
        "\n",
        "print(\"\\nMobileNetV2:\\n\")\n",
        "evaluate_model(MobileNetV2, validation_dataset)\n",
        "evaluate_tflite_model(MobileNetV2_tflite, validation_dataset)\n",
        "save_model(MobileNetV2_tflite, \"MobileNetV2.tflite\")\n",
        "\n",
        "print(\"\\nEfficientNetB0:\\n\")\n",
        "evaluate_model(EfficientNetB0, validation_dataset)\n",
        "evaluate_tflite_model(EfficientNetB0_tflite, validation_dataset)\n",
        "save_model(nEfficientNetB0_tflite, \"nEfficientNetB0.tflite\")\n",
        "\n",
        "print(\"\\nResNet101:\\n\")\n",
        "evaluate_model(ResNet101, validation_dataset)\n",
        "evaluate_tflite_model(ResNet101_tflite, validation_dataset)\n",
        "save_model(ResNet101_tflite, \"ResNet101.tflite\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 11005152,
          "datasetId": 6602109,
          "sourceId": 10661516,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "py3_11_11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
