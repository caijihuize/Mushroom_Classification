{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Exon7Oj4z-g0",
        "outputId": "55b39730-2c6a-4a97-c8ec-ac5b67a4295f"
      },
      "outputs": [],
      "source": [
        "# 基础库\n",
        "import os                  # 提供与操作系统交互的功能，如文件路径操作\n",
        "import platform            # 获取系统平台信息\n",
        "import sys                 # 提供对Python解释器的访问和控制\n",
        "import urllib.request      # 用于处理URL和网络请求\n",
        "\n",
        "# 数据处理和科学计算\n",
        "import numpy as np         # 科学计算库，提供多维数组和矩阵运算\n",
        "import tensorflow as tf    # 深度学习框架，用于构建和训练神经网络\n",
        "\n",
        "# 数据可视化\n",
        "import matplotlib.pyplot as plt          # 绘图库，用于创建静态、动态和交互式可视化\n",
        "from matplotlib.font_manager import FontProperties, fontManager  # 字体管理，支持中文显示\n",
        "\n",
        "# Jupyter相关\n",
        "from IPython.display import clear_output  # 用于清除Jupyter notebook输出\n",
        "\n",
        "# Kaggle相关\n",
        "import kagglehub          # Kaggle数据集管理工具，用于下载和访问Kaggle数据集\n",
        "\n",
        "\n",
        "# 打印Python环境信息\n",
        "print(\"Python version:\", sys.version)     # 打印Python版本\n",
        "print(\"Platform:\", platform.platform())   # 打印操作系统平台（完整信息）\n",
        "print(\"System:\", platform.system())       # 打印操作系统类型（如Windows、Linux）\n",
        "print(\"Node:\", platform.node())           # 打印网络节点名称（主机名）\n",
        "print(\"Release:\", platform.release())     # 打印操作系统发行版本\n",
        "print(\"Version:\", platform.version())     # 打印操作系统版本号（更详细）\n",
        "print(\"Machine:\", platform.machine())     # 打印计算机类型（如x86_64）\n",
        "print(\"Processor:\", platform.processor()) # 打印处理器类型\n",
        "\n",
        "# 检查TensorFlow版本及其依赖\n",
        "!pip show tensorflow                      # 显示TensorFlow包的详细信息，包括版本和依赖\n",
        "\n",
        "# 检查系统硬件信息\n",
        "!cat /proc/cpuinfo | grep 'model name'    # 显示CPU型号（Linux系统）\n",
        "!cat /proc/meminfo | grep 'MemTotal'      # 显示系统总内存大小（Linux系统）\n",
        "!df -h                                    # 显示磁盘使用情况（disk free，人类可读格式）\n",
        "!nvidia-smi                               # 显示NVIDIA GPU信息（如果系统有NVIDIA GPU）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL7FdUwf7_M1"
      },
      "source": [
        "# 数据集\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJmst4Ck8UIJ"
      },
      "source": [
        "## 数据集获取\n",
        " \n",
        "本项目使用的蘑菇图像数据集托管在 [Kaggle](https://www.kaggle.com/) 平台上，数据集名称为 [huizecai/mushroom](https://www.kaggle.com/datasets/huizecai/mushroom)。该数据集包含了多种常见蘑菇的高清图片，以及对应的分类标签。\n",
        "\n",
        "为了方便数据获取，我们使用 `kagglehub` 库来自动下载和管理数据集。下面的代码单元格会直接从 Kaggle 下载数据集，并返回保存在本地的路径。数据集下载完成后会被缓存，后续运行时将直接使用缓存版本，无需重复下载。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE-AmgP8cAmb",
        "outputId": "4c30f603-73a3-4078-aacf-7ce08f7e1f63",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 设置数据集名称\n",
        "dataset_name = \"huizecai/mushroom\"  # 指定要下载的Kaggle数据集名称\n",
        "\n",
        "# 使用KaggleHub下载数据集\n",
        "path = kagglehub.dataset_download(dataset_name)  # 下载数据集并获取保存路径\n",
        "\n",
        "# 打印数据集文件的保存路径\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# 设置数据和标签文件的具体路径\n",
        "dataset_path = path + '/archive/data'  # 图片数据所在目录的路径\n",
        "label_path = path + '/archive/label.txt'  # 标签文件的路径"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVChVsT_97wJ"
      },
      "source": [
        "## 数据集类别统计分析\n",
        " \n",
        "为了避免TensorFlow处理中文路径时可能出现的编码问题，本数据集采用了规范化的命名方式:\n",
        " - 各蘑菇种类的文件夹以\"classXX\"格式命名(XX为数字编号)\n",
        " - 使用label.txt文件建立文件夹编号与中文名称的映射关系\n",
        " - 这种设计既保证了系统兼容性，又方便了数据的管理和使用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN4YxYi5cAmc",
        "outputId": "8cb66590-d88b-4b99-a0cb-863ff6fdebf9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 获取所有子目录（即蘑菇类别）\n",
        "# os.listdir() 列出指定目录下的所有文件和文件夹\n",
        "# os.path.isdir() 判断是否为文件夹\n",
        "# 使用列表推导式获取所有蘑菇类别的文件夹名\n",
        "dir_names = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "\n",
        "# 读取 label.txt 并解析内容\n",
        "# 创建一个空字典用于存储类别ID和名称的映射关系\n",
        "categories = {}\n",
        "# 以UTF-8编码打开label.txt文件\n",
        "with open(label_path, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        # 去除每行首尾空白字符并按空格分割\n",
        "        parts = line.strip().split()\n",
        "        # 确保每行包含两个部分:类别名称和ID\n",
        "        if len(parts) == 2:\n",
        "            category_name = parts[0]  # 第一部分为类别名称(中文)\n",
        "            category_id = parts[1]    # 第二部分为类别ID\n",
        "            categories[category_id] = category_name  # 建立ID到名称的映射\n",
        "\n",
        "# 统计每种类别的图像数量\n",
        "# 创建空字典存储每个类别的图片数量\n",
        "category_counts = {}\n",
        "for category_id in categories.keys():\n",
        "    # 确保目录存在再进行统计\n",
        "    if category_id in dir_names:\n",
        "        # 构建完整的类别目录路径\n",
        "        category_dir = os.path.join(dataset_path, category_id)\n",
        "        # 统计jpg和jpeg格式的图片数量\n",
        "        # 使用列表推导式过滤出图片文件并计数\n",
        "        num_images = len([f for f in os.listdir(category_dir) if f.endswith('.jpg') or f.endswith('.jpeg')])\n",
        "        # 使用中文类别名称作为键存储图片数量\n",
        "        category_counts[categories[category_id]] = num_images\n",
        "\n",
        "# 打印每个类别的图片数量统计结果\n",
        "print(\"Category counts:\", category_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i374m8Jd-ob6"
      },
      "source": [
        "## 解决matplotlib中文显示问题\n",
        "\n",
        "matplotlib默认不支持中文字体显示,可能会出现乱码。为了确保数据可视化结果能正确展示中文:\n",
        "1. 我们将下载并使用\"SimHei\"(黑体)字体\n",
        "2. 注册字体到matplotlib的字体管理器\n",
        "3. 配置全局字体设置\n",
        "\n",
        "这样可以保证后续所有图表中的中文标题、标签等都能正常显示。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "Hv_Ltg1xcAmd",
        "outputId": "4e8a29c4-5b7b-4604-9f7c-ed8812bf0e6e"
      },
      "outputs": [],
      "source": [
        "# 创建img目录(如果不存在)\n",
        "if not os.path.exists('img'):\n",
        "    os.makedirs('img')\n",
        "\n",
        "# 设置字体文件的URL和本地保存路径\n",
        "font_url = \"https://github.com/caijihuize/Mushroom_Classification/raw/main/SimHei.ttf\"  # 黑体字体文件的URL\n",
        "font_name = \"SimHei.ttf\"  # 本地保存的字体文件名\n",
        "\n",
        "# 如果字体文件不存在则下载\n",
        "if not os.path.exists(font_name):\n",
        "    urllib.request.urlretrieve(font_url, font_name)  # 下载字体文件到本地\n",
        "\n",
        "# 配置matplotlib的字体设置\n",
        "fontManager.addfont(font_name)  # 将字体文件添加到matplotlib的字体管理器\n",
        "font_prop = FontProperties(fname=font_name)  # 创建字体属性对象\n",
        "\n",
        "# 设置全局字体配置\n",
        "plt.rcParams['font.family'] = 'SimHei'  # 设置默认字体为黑体\n",
        "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
        "plt.rcParams['font.size'] = 20  # 设置全局字体大小\n",
        "plt.rcParams['axes.titlesize'] = 22  # 设置标题字体大小\n",
        "plt.rcParams['axes.labelsize'] = 20  # 设置轴标签字体大小\n",
        "plt.rcParams['figure.dpi'] = 300  # 设置图形DPI为300,提高显示清晰度\n",
        "plt.rcParams['savefig.dpi'] = 600  # 设置保存图片的DPI为600,提高保存图片的清晰度\n",
        "\n",
        "# 绘制测试图表验证中文显示\n",
        "plt.figure(figsize=(10, 6))  # 设置图形大小\n",
        "plt.title('这是一个标题', fontsize=20)  # 设置标题\n",
        "plt.xlabel('X轴标签', fontsize=16)  # 设置X轴标签\n",
        "plt.ylabel('Y轴标签', fontsize=16)  # 设置Y轴标签\n",
        "plt.plot([0, 1, 2, 3], [0, 1, 4, 9], linewidth=2)  # 绘制简单的折线图,增加线宽提高清晰度\n",
        "\n",
        "# 保存图表到img目录,使用更高质量的设置\n",
        "plt.savefig('img/test_plot.png', \n",
        "            bbox_inches='tight',  # 自动调整边界\n",
        "            dpi=800,  # 设置更高的DPI\n",
        "            format='png',  # 使用PNG格式保存\n",
        "            facecolor='white',  # 设置白色背景\n",
        "            edgecolor='none',  # 无边框\n",
        "            transparent=False)  # 不透明\n",
        "plt.show()  # 显示图表"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-jZkqsW_tQ6"
      },
      "source": [
        "## 绘制各种类图片数量的柱状图"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "9aAf366McAmd",
        "outputId": "5fa50055-8c01-48be-c3f3-99f06df8c517",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 准备数据\n",
        "categories_readable = list(category_counts.keys())  # 获取所有蘑菇种类名称\n",
        "counts = list(category_counts.values())  # 获取每个种类对应的图片数量\n",
        "\n",
        "# 创建一个新的图形，设置更大的尺寸以便更好地展示数据\n",
        "plt.figure(figsize=(18, 16))\n",
        "\n",
        "# 创建颜色渐变 - 根据数量排序，数量越多颜色越亮\n",
        "# 首先创建数量和类别的映射关系\n",
        "count_category_pairs = list(zip(counts, categories_readable))\n",
        "# 按照数量排序\n",
        "count_category_pairs.sort(key=lambda x: x[0])\n",
        "# 提取排序后的类别和数量\n",
        "sorted_categories = [pair[1] for pair in count_category_pairs]\n",
        "sorted_counts = [pair[0] for pair in count_category_pairs]\n",
        "\n",
        "# 创建颜色映射，使用较为柔和的颜色差异\n",
        "# 使用单一色系的渐变，减小颜色差异\n",
        "norm = plt.Normalize(min(counts), max(counts))\n",
        "# 使用Blues色系，颜色差异较小\n",
        "colors = plt.cm.Blues(norm(counts) * 0.7 + 0.3)  # 缩小颜色范围，增加最小值，减小颜色差异\n",
        "\n",
        "# 绘制水平柱状图，使用柔和的颜色\n",
        "bars = plt.barh(categories_readable, counts, color=colors, height=0.7, \n",
        "                edgecolor='gray', linewidth=0.5, alpha=0.9)  # 增加透明度使颜色更柔和\n",
        "\n",
        "# 在每个柱子右侧添加数值标签\n",
        "for i, bar in enumerate(bars):\n",
        "    width = bar.get_width()  # 获取柱子的宽度(即图片数量)\n",
        "    plt.text(width + 2, bar.get_y() + bar.get_height()/2, f'{int(width)}', \n",
        "             va='center', ha='left', fontsize=14, fontweight='bold', \n",
        "             color='darkblue')  # 美化标签样式\n",
        "\n",
        "# 设置图表标题和轴标签\n",
        "plt.title('各蘑菇种类的图片数量分布', fontsize=28, fontweight='bold', pad=20)  # 增大标题字体并加粗\n",
        "plt.xlabel('图片数量', fontsize=22, labelpad=15)  # 设置x轴标签并增加内边距\n",
        "plt.ylabel('蘑菇种类', fontsize=22, labelpad=15)  # 设置y轴标签并增加内边距\n",
        "\n",
        "# 设置坐标轴样式\n",
        "plt.tick_params(axis='both', which='major', labelsize=16)  # 增大刻度标签字体\n",
        "plt.xlim(25, max(counts) + max(counts)*0.1)  # 设置x轴范围，留出更多空间\n",
        "\n",
        "# 添加网格线\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.4, color='gray')  # 使用更淡的网格线\n",
        "\n",
        "# 添加背景色\n",
        "plt.gca().set_facecolor('#f8f9fa')  # 设置浅灰色背景\n",
        "plt.gca().spines['top'].set_visible(False)  # 移除上边框\n",
        "plt.gca().spines['right'].set_visible(False)  # 移除右边框\n",
        "plt.gca().spines['left'].set_linewidth(0.5)  # 减小左边框宽度\n",
        "plt.gca().spines['bottom'].set_linewidth(0.5)  # 减小下边框宽度\n",
        "\n",
        "# 自动调整布局，防止标签被截断\n",
        "plt.tight_layout()\n",
        "\n",
        "# 保存图表到img目录，使用更高质量的设置\n",
        "plt.savefig('img/mushroom_distribution.png', \n",
        "            bbox_inches='tight', \n",
        "            dpi=800, \n",
        "            facecolor='#f8f9fa')  # 保存高质量图片，保持背景色一致\n",
        "\n",
        "# 显示图形\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP2oyCdcDePo"
      },
      "source": [
        "# 训练准备工作"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IInl-UREEOOr"
      },
      "source": [
        "## 加载图像数据集\n",
        "\n",
        "使用 TensorFlow 的 [image_dataset_from_directory](https://tensorflow.google.cn/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory?hl=en) 函数加载和准备图像数据集：\n",
        "\n",
        "\n",
        "*   directory=dataset_path ：指定图像数据所在的路径。\n",
        "*   image_size=(224, 224) ：指定每个图像的大小为224x224像素。\n",
        "*   batch_size=32 ：指定每个批次包含32张图像。\n",
        "*   validation_split=0.1 ：指定10%的数据作为验证集。\n",
        "*   subset='both' ：指定同时返回训练集和验证集。\n",
        "*   label_mode='categorical' ：指定标签模式为分类模式，返回one-hot编码的标签。\n",
        "*   seed=21 ：设置随机种子以确保数据集的可重复性。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jxk-SQQ-fiNv",
        "outputId": "e403800a-0a79-4bdd-d584-f51905c37f79"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 加载和准备图像数据集\n",
        "train_dataset, validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=dataset_path,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    subset='both',\n",
        "    label_mode='categorical',\n",
        "    seed=44\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iVqn2bVNewl"
      },
      "source": [
        "## 计算训练集和验证集中各类别图像的分布情况\n",
        "\n",
        "下面我们将统计训练集和验证集中每个蘑菇类别的图像数量，以便了解数据集的分布特征。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0a6DQ-dIQSN",
        "outputId": "510f098d-7cd7-4fc3-bc7f-cc5df74dfb91"
      },
      "outputs": [],
      "source": [
        "# 获取类别名称\n",
        "class_names = train_dataset.class_names\n",
        "# 从训练数据集中获取所有类别的名称列表\n",
        "\n",
        "# 初始化字典用于存储每种类别的图像数量\n",
        "train_category_counts = {name: 0 for name in categories.values()}\n",
        "validation_category_counts = {name: 0 for name in categories.values()}\n",
        "# 创建两个字典，分别用于存储训练集和验证集中每个类别的图像数量\n",
        "# 使用字典推导式初始化，键为类别的中文名称，值初始化为0\n",
        "\n",
        "# 统计训练集中的图像数量\n",
        "for images, labels in train_dataset:\n",
        "    # 遍历训练数据集中的每个批次，每个批次包含图像和对应的标签\n",
        "    for label in labels.numpy():\n",
        "        # 将标签张量转换为numpy数组并遍历\n",
        "        category_name = class_names[np.argmax(label)]\n",
        "        # np.argmax(label)找出one-hot编码中值为1的索引位置\n",
        "        # 通过索引从class_names中获取对应的类别名称\n",
        "        train_category_counts[categories[category_name]] += 1\n",
        "        # 将该类别在训练集中的计数加1，使用categories字典将英文名映射为中文名\n",
        "\n",
        "# 统计验证集中的图像数量\n",
        "for images, labels in validation_dataset:\n",
        "    # 遍历验证数据集中的每个批次\n",
        "    for label in labels.numpy():\n",
        "        # 同样处理验证集中的标签\n",
        "        category_name = class_names[np.argmax(label)]\n",
        "        validation_category_counts[categories[category_name]] += 1\n",
        "        # 将该类别在验证集中的计数加1\n",
        "\n",
        "# 打印统计结果\n",
        "print(\"训练集类别图像数量统计:\", train_category_counts)\n",
        "print(\"验证集类别图像数量统计:\", validation_category_counts)\n",
        "# 输出训练集和验证集中各个类别的图像数量统计结果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9PmfburNand"
      },
      "source": [
        "## 绘制训练集和验证集的柱状图"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "VR2URGk4JLd8",
        "outputId": "d10c2735-93cc-493a-fe3f-8ac5446ee1b8"
      },
      "outputs": [],
      "source": [
        "# 准备数据\n",
        "categories_readable = list(categories.values())  # 获取所有蘑菇类别的中文名称列表\n",
        "train_counts = [train_category_counts[name] for name in categories_readable]  # 获取每个类别在训练集中的图片数量\n",
        "validation_counts = [validation_category_counts[name] for name in categories_readable]  # 获取每个类别在验证集中的图片数量\n",
        "\n",
        "# 创建一个新的图形\n",
        "plt.figure(figsize=(14, 8))  # 设置图形大小为14x8英寸\n",
        "\n",
        "# 设置柱子的位置\n",
        "bar_width = 0.35  # 设置柱子的宽度\n",
        "index = np.arange(len(categories_readable))  # 创建类别索引数组\n",
        "\n",
        "# 绘制训练集柱状图\n",
        "bars_train = plt.barh(index + bar_width, train_counts, bar_width, \n",
        "                     label='训练集', color='skyblue')  # 绘制训练集的水平柱状图\n",
        "\n",
        "# 绘制验证集柱状图\n",
        "bars_validation = plt.barh(index, validation_counts, bar_width, \n",
        "                         label='测试集', color='orange')  # 绘制验证集的水平柱状图\n",
        "\n",
        "# 添加数据标签的函数\n",
        "def add_labels(bars):\n",
        "    \"\"\"为每个柱子添加数值标签\"\"\"\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()  # 获取柱子的宽度(即数值)\n",
        "        plt.text(width + 1,  # x坐标(数值右侧)\n",
        "                bar.get_y() + bar.get_height() / 2,  # y坐标(柱子中间)\n",
        "                '%d' % int(width),  # 显示的文本(整数格式)\n",
        "                ha='left', va='center')  # 文本对齐方式\n",
        "\n",
        "# 为训练集和验证集的柱子添加标签\n",
        "add_labels(bars_train)\n",
        "add_labels(bars_validation)\n",
        "\n",
        "# 设置图表标题和轴标签\n",
        "plt.title('各蘑菇种类的图片数量 (Train vs Validation)', fontsize=20)\n",
        "plt.xlabel('图片数量', fontsize=16)\n",
        "plt.ylabel('蘑菇种类', fontsize=16)\n",
        "\n",
        "# 设置Y轴刻度标签为蘑菇类别名称\n",
        "plt.yticks(index + bar_width / 2, categories_readable)\n",
        "\n",
        "# 显示网格线\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)  # 只显示x轴方向的网格线\n",
        "\n",
        "# 添加图例\n",
        "plt.legend()\n",
        "\n",
        "# 自动调整布局，防止标签被截断\n",
        "plt.tight_layout()\n",
        "\n",
        "# 保存图形\n",
        "plt.savefig('img/mushroom_distribution_vs.png', dpi=800, bbox_inches='tight')\n",
        "\n",
        "# 显示图形\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3psFiWZHOXw"
      },
      "source": [
        "## 显示数据集中的图像样本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "9lZeeEEgesTm",
        "outputId": "b9dfcb59-9b83-4372-fa27-df778c7e070f"
      },
      "outputs": [],
      "source": [
        "# 获取训练数据集中的类别名称\n",
        "class_names = train_dataset.class_names\n",
        "\n",
        "# 设置要在图中显示的随机样本图像数量 \n",
        "num_images_to_show = 4\n",
        "\n",
        "# 初始化存储图像和标签的列表\n",
        "images_to_display = []  # 用于存储待显示的图像\n",
        "labels_to_display = []  # 用于存储对应的标签\n",
        "\n",
        "# 从训练数据集中随机抽取一批数据\n",
        "for images, labels in train_dataset.take(1):\n",
        "    # 随机选择指定数量的图像索引,不重复\n",
        "    indices = np.random.choice(range(images.shape[0]), \n",
        "                             num_images_to_show, \n",
        "                             replace=False)\n",
        "    \n",
        "    # 根据随机索引获取对应的图像和标签\n",
        "    for index in indices:\n",
        "        images_to_display.append(images[index])\n",
        "        labels_to_display.append(labels[index])\n",
        "\n",
        "# 创建1行num_images_to_show列的子图网格\n",
        "fig, axes = plt.subplots(1, num_images_to_show, figsize=(12, 4))\n",
        "\n",
        "# 遍历显示每张图像\n",
        "for i, (image, label) in enumerate(zip(images_to_display, labels_to_display)):\n",
        "    ax = axes[i]\n",
        "    # 将图像数据转换为uint8类型并显示\n",
        "    ax.imshow(image.numpy().astype(\"uint8\"))\n",
        "    # 设置图像标题为对应的蘑菇类别名称\n",
        "    ax.set_title(categories[class_names[np.argmax(label.numpy())]], \n",
        "                fontsize=20)\n",
        "    # 关闭坐标轴显示\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# 自动调整子图之间的间距\n",
        "plt.tight_layout()\n",
        "\n",
        "# 保存图形\n",
        "plt.savefig('img/mushroom_samples.png', dpi=800, bbox_inches='tight')\n",
        "\n",
        "# 显示整个图形\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3cwpuE9O5nP"
      },
      "source": [
        "# 数据增强\n",
        "\n",
        "数据增强是一种常用的技术，用于通过变换现有数据来增加训练数据的多样性，从而提高模型的泛化能力。在图像处理中，常见的数据增强技术包括旋转、翻转、缩放、裁剪、亮度调整等。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW2ujx4-RvQg"
      },
      "source": [
        "## 定义数据增强层"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "n1bKrejhgePO"
      },
      "outputs": [],
      "source": [
        "# 定义数据增强的预处理层\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),  # 水平翻转\n",
        "    tf.keras.layers.RandomRotation(0.2),       # 随机旋转最多20%\n",
        "    tf.keras.layers.RandomZoom(0.2, 0.2),      # 随机缩放\n",
        "    tf.keras.layers.RandomContrast(0.2),       # 随机对比度调整\n",
        "    tf.keras.layers.RandomBrightness(0.2)      # 随机亮度调整\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I53w8VKuRz9t"
      },
      "source": [
        "## 定义显示数据增强示例函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-FayHXVxtypj"
      },
      "outputs": [],
      "source": [
        "def demo_augmentation(sample_image, model, num_aug):\n",
        "    '''接收单个图像数组，然后使用模型生成num_aug个数据增强变换'''\n",
        "\n",
        "    # 初始化预览图像列表\n",
        "    image_preview = []\n",
        "\n",
        "    # 将输入图像转换为PIL图像实例\n",
        "    sample_image_pil = tf.keras.utils.array_to_img(sample_image)\n",
        "\n",
        "    # 将原始图像添加到列表中\n",
        "    image_preview.append(sample_image_pil)\n",
        "\n",
        "    # 应用图像增强并将结果添加到列表中\n",
        "    for i in range(NUM_AUG):\n",
        "        # 扩展维度以适应模型输入\n",
        "        sample_image_aug = model(tf.expand_dims(sample_image, axis=0))\n",
        "        # 将增强后的图像转换为PIL格式\n",
        "        sample_image_aug_pil = tf.keras.utils.array_to_img(tf.squeeze(sample_image_aug))\n",
        "        image_preview.append(sample_image_aug_pil)\n",
        "\n",
        "    # 创建子图布局\n",
        "    fig, axes = plt.subplots(1, NUM_AUG + 1, figsize=(12, 12))\n",
        "\n",
        "    # 显示所有图像\n",
        "    for index, ax in enumerate(axes):\n",
        "        ax.imshow(image_preview[index])\n",
        "        ax.set_axis_off()\n",
        "\n",
        "        # 设置图像标题\n",
        "        if index == 0:\n",
        "            ax.set_title('原始图像')\n",
        "        else:\n",
        "            ax.set_title(f'增强样本 {index}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvJSBnm7SK5c"
      },
      "source": [
        "## 获取随机批次"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55HfA-g5uG8o",
        "outputId": "698265b2-710b-46ce-ad38-401e890c1554"
      },
      "outputs": [],
      "source": [
        "# 获取训练集中随机的一批次图片\n",
        "sample_batch = list(train_dataset.take(1))[0][0]\n",
        "print(f'images per batch: {len(sample_batch)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7YzAYs5VBUq"
      },
      "source": [
        "## 显示数据增强示例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "mfeF4w3Qt725",
        "outputId": "0c03a95c-8411-4c6e-994f-69efda80073f"
      },
      "outputs": [],
      "source": [
        "# 设置每个样本的数据增强次数\n",
        "NUM_AUG = 5\n",
        "\n",
        "# 对训练集中前4张图片进行数据增强演示\n",
        "# 使用demo_augmentation函数对每张图片进行NUM_AUG次数据增强\n",
        "# 并将原始图片和增强后的图片并排显示\n",
        "demo_augmentation(sample_batch[0], data_augmentation, NUM_AUG)  # 处理第1张图片\n",
        "demo_augmentation(sample_batch[1], data_augmentation, NUM_AUG)  # 处理第2张图片 \n",
        "demo_augmentation(sample_batch[2], data_augmentation, NUM_AUG)  # 处理第3张图片\n",
        "demo_augmentation(sample_batch[3], data_augmentation, NUM_AUG)  # 处理第4张图片"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMccJ_imamqA"
      },
      "source": [
        "# 模型训练相关函数定义\n",
        "以下定义了用于模型训练的核心函数,包括:\n",
        "- train_model: 训练深度学习模型的主函数,负责模型编译、配置回调函数和训练过程\n",
        "- 回调函数:\n",
        "  - early_stopping: 提前停止训练,防止过拟合\n",
        "  - lr_scheduler: 动态调整学习率,优化训练效果\n",
        "- 训练参数:\n",
        "  - optimizer: adam优化器\n",
        "  - loss: 分类交叉熵损失函数\n",
        "  - metrics: 准确率评估指标"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "I9GNzqANFicv"
      },
      "outputs": [],
      "source": [
        "# 训练模型函数\n",
        "def train_model(model, train_dataset=train_dataset, validation_dataset=validation_dataset, epochs=30):\n",
        "    \"\"\"\n",
        "    训练深度学习模型的函数\n",
        "    \n",
        "    参数:\n",
        "        model: 待训练的模型\n",
        "        train_dataset: 训练数据集\n",
        "        validation_dataset: 验证数据集  \n",
        "        epochs: 训练轮数,默认30轮\n",
        "        \n",
        "    返回:\n",
        "        history: 训练历史记录,包含loss和accuracy等指标\n",
        "    \"\"\"\n",
        "    # 配置模型训练参数\n",
        "    # 使用adam优化器,交叉熵损失函数,准确率评估指标\n",
        "    model.compile(optimizer='adam', \n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "    # 配置回调函数\n",
        "    # early_stopping: 当验证集loss在6轮内没有改善时停止训练,并恢复最佳权重\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=6, \n",
        "        restore_best_weights=True\n",
        "    )\n",
        "    \n",
        "    # lr_scheduler: 当验证集loss在3轮内没有改善时,学习率减半,最小到1e-7\n",
        "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-7\n",
        "    )\n",
        "\n",
        "    # 开始训练\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=epochs,\n",
        "        validation_data=validation_dataset, \n",
        "        callbacks=[early_stopping, lr_scheduler]\n",
        "    )\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 模型量化和保存\n",
        " \n",
        "为了在移动设备和嵌入式系统上高效部署模型,我们需要:\n",
        "1. 将训练好的Keras模型量化 - 把权重从float32转为int8,显著减小模型大小\n",
        "2. 转换为TFLite格式 - TFLite是专门为移动和嵌入式设备优化的轻量级框架\n",
        "3. 保存模型到文件 - 以便后续部署使用\n",
        "下面两个函数实现了这些功能:\n",
        "- quantize_and_convert_to_tflite(): 模型量化和TFLite转换\n",
        "- save_model(): 保存模型到文件\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hcr0xToSWwun"
      },
      "outputs": [],
      "source": [
        "def quantize_and_convert_to_tflite(model):\n",
        "    \"\"\"\n",
        "    将Keras模型量化并转换为TFLite格式\n",
        "    \n",
        "    参数:\n",
        "        model: Keras模型对象\n",
        "        \n",
        "    返回:\n",
        "        tflite_quant_model: 量化后的TFLite模型\n",
        "        \n",
        "    说明:\n",
        "        - 使用TFLite转换器将Keras模型转换为TFLite格式\n",
        "        - 启用默认优化以减小模型大小和提高推理性能\n",
        "        - 量化可以将模型权重从float32转换为int8,显著减小模型大小\n",
        "    \"\"\"\n",
        "    # 创建TFLite转换器\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    # 启用默认优化(包括权重量化)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    # 执行转换\n",
        "    tflite_quant_model = converter.convert()\n",
        "    \n",
        "    return tflite_quant_model\n",
        "\n",
        "def save_model(model, output_path):\n",
        "    \"\"\"\n",
        "    保存模型到文件\n",
        "    \n",
        "    参数:\n",
        "        model: 要保存的模型对象\n",
        "        output_path: 保存模型的文件路径\n",
        "        \n",
        "    说明:\n",
        "        - 以二进制格式写入模型到指定路径\n",
        "        - 通常用于保存TFLite模型(.tflite文件)\n",
        "    \"\"\"\n",
        "    with open(output_path, 'wb') as f:\n",
        "        f.write(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "可视化训练历史\n",
        "\n",
        "下面的函数用于可视化模型训练过程中的指标变化:\n",
        "\n",
        "- show_history(): 绘制训练历史中的损失和准确率曲线\n",
        "  - 创建1x2的子图布局\n",
        "  - 左图显示训练和验证损失\n",
        "  - 右图显示训练和验证准确率\n",
        "  - 使用matplotlib进行绘图\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "magj2cqVUiZJ"
      },
      "outputs": [],
      "source": [
        "def show_history(history):\n",
        "    \"\"\"\n",
        "    可视化模型训练历史中的损失和准确率曲线\n",
        "    \n",
        "    参数:\n",
        "        history: 模型训练返回的History对象,包含训练过程中的指标数据\n",
        "        \n",
        "    功能:\n",
        "        - 创建一个1x2的子图布局\n",
        "        - 左图显示训练集和验证集的损失曲线\n",
        "        - 右图显示训练集和验证集的准确率曲线\n",
        "        - 自动调整布局并显示图形\n",
        "        \n",
        "    返回:\n",
        "        无返回值,直接显示图形\n",
        "    \"\"\"\n",
        "    # 创建12x4英寸的画布\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # 绘制损失曲线\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # 绘制准确率曲线\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # 自动调整子图布局\n",
        "    plt.tight_layout()\n",
        "    # 显示图形\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dVpHWE1qwPKp"
      },
      "outputs": [],
      "source": [
        "def combine_history(histories, labels):\n",
        "    \"\"\"\n",
        "    将多个模型的训练历史进行对比可视化并保存图片\n",
        "    \n",
        "    参数:\n",
        "        histories: 列表，包含多个模型训练返回的History对象\n",
        "        labels: 列表，每个模型对应的标签名称\n",
        "        \n",
        "    功能:\n",
        "        - 找出所有模型中最小的训练轮数\n",
        "        - 创建1x2的子图布局\n",
        "        - 左图对比各模型的训练集和验证集损失曲线\n",
        "        - 右图对比各模型的训练集和验证集准确率曲线\n",
        "        - 使用不同线型区分训练集和验证集\n",
        "        - 添加图例和网格线增强可读性\n",
        "        - 保存图片到本地\n",
        "    \"\"\"\n",
        "    # 找出所有模型中最小的训练轮数,用于对齐显示\n",
        "    min_epochs = min([len(history.history['loss']) for history in histories])\n",
        "    epochs = range(1, min_epochs + 1)\n",
        "\n",
        "    # 创建14x6英寸的画布\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # 绘制损失曲线对比图\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for i, history in enumerate(histories):\n",
        "        # 截取相同长度的训练历史数据\n",
        "        train_loss = history.history['loss'][:min_epochs]\n",
        "        val_loss = history.history['val_loss'][:min_epochs]\n",
        "        # 使用虚线表示训练集,实线表示验证集\n",
        "        plt.plot(epochs, train_loss, label=f'{labels[i]} Train Loss', linestyle='--')\n",
        "        plt.plot(epochs, val_loss, label=f'{labels[i]} Val Loss')\n",
        "\n",
        "    plt.title('Training and Validation Loss Comparison')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # 绘制准确率曲线对比图\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for i, history in enumerate(histories):\n",
        "        # 截取相同长度的训练历史数据\n",
        "        train_acc = history.history['accuracy'][:min_epochs]\n",
        "        val_acc = history.history['val_accuracy'][:min_epochs]\n",
        "        # 使用虚线表示训练集,实线表示验证集\n",
        "        plt.plot(epochs, train_acc, label=f'{labels[i]} Train Accuracy', linestyle='--')\n",
        "        plt.plot(epochs, val_acc, label=f'{labels[i]} Val Accuracy')\n",
        "\n",
        "    plt.title('Training and Validation Accuracy Comparison')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # 自动调整子图布局\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # 保存图片\n",
        "    plt.savefig('img/model_comparison.png', dpi=1000, bbox_inches='tight')\n",
        "    \n",
        "    # 显示图形\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "geSO-2Iyn777"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def calculate_class_accuracies(model, dataset):\n",
        "    \"\"\"\n",
        "    计算并可视化每个类别的准确率\n",
        "    \n",
        "    参数:\n",
        "        model: 训练好的模型\n",
        "        dataset: 数据集\n",
        "        \n",
        "    返回:\n",
        "        dict: 每个类别的准确率字典\n",
        "    \"\"\"\n",
        "    # 收集所有预测结果和真实标签\n",
        "    all_predictions = []\n",
        "    all_true_labels = []\n",
        "\n",
        "    for images, labels in dataset:\n",
        "        predictions = model.predict(images)\n",
        "        predicted_classes = np.argmax(predictions, axis=1) \n",
        "        true_classes = np.argmax(labels, axis=1)\n",
        "        \n",
        "        all_predictions.extend(predicted_classes)\n",
        "        all_true_labels.extend(true_classes)\n",
        "\n",
        "    # 计算每个类别的准确率\n",
        "    class_accuracies = {}\n",
        "    for class_index in range(len(class_names)):\n",
        "        mask = (np.array(all_true_labels) == class_index)\n",
        "        correct_predictions = np.sum(np.array(all_predictions)[mask] == class_index)\n",
        "        total_samples = np.sum(mask)\n",
        "        if total_samples > 0:\n",
        "            accuracy = correct_predictions / total_samples\n",
        "        else:\n",
        "            accuracy = 0\n",
        "        class_accuracies[class_names[class_index]] = accuracy\n",
        "\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # 准备绘图数据\n",
        "    categories_readable = list(categories[class_name] for class_name in class_accuracies.keys())\n",
        "    accuracies = list(class_accuracies.values())\n",
        "\n",
        "    # 创建水平柱状图\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    bars = plt.barh(categories_readable, accuracies, color='skyblue')\n",
        "\n",
        "    # 在柱状图上添加数值标签\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()\n",
        "        plt.text(width + 0.01, bar.get_y() + bar.get_height()/2, f'{width:.4f}', va='center')\n",
        "\n",
        "    # 设置图表标题和标签\n",
        "    plt.title('各类别准确率', fontsize=20)\n",
        "    plt.xlabel('准确率', fontsize=16)\n",
        "    plt.ylabel('类别', fontsize=16)\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # 保存图片\n",
        "    plt.savefig('class_accuracies.png', dpi=800, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return class_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1YgDWOLmDn3J"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataset):\n",
        "    \"\"\"使用TensorFlow内置metrics评估Keras模型在数据集上的性能\n",
        "    \n",
        "    Args:\n",
        "        model: Keras模型对象\n",
        "        dataset: tf.data.Dataset数据集对象\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (top1准确率, top5准确率, 模型大小, 推理速度)\n",
        "    \"\"\"\n",
        "    # 计算模型大小 (MB)\n",
        "    model_size_mb = sum(np.prod(w.shape) for w in model.get_weights()) * 4 / (1024 * 1024)  \n",
        "\n",
        "    # 创建评估指标\n",
        "    top1_accuracy = tf.keras.metrics.CategoricalAccuracy()  # Top-1准确率评估器\n",
        "    top5_accuracy = tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_accuracy')  # Top-5准确率评估器\n",
        "\n",
        "    # 创建损失函数和平均损失计算器\n",
        "    loss_object = tf.keras.losses.CategoricalCrossentropy()  # 分类交叉熵损失函数\n",
        "    loss_avg = tf.keras.metrics.Mean()  # 用于计算平均损失\n",
        "\n",
        "    # 记录推理时间\n",
        "    total_time = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # 在数据集上运行评估\n",
        "    for images, labels in dataset:\n",
        "        start_time = time.time()\n",
        "        predictions = model(images, training=False)  # 获取模型预测结果\n",
        "        end_time = time.time()\n",
        "        \n",
        "        # 累计推理时间和样本数\n",
        "        total_time += end_time - start_time\n",
        "        total_samples += len(images)\n",
        "        \n",
        "        loss = loss_object(labels, predictions)  # 计算损失值\n",
        "\n",
        "        # 更新评估指标\n",
        "        loss_avg.update_state(loss)\n",
        "        top1_accuracy.update_state(labels, predictions)\n",
        "        top5_accuracy.update_state(labels, predictions)\n",
        "\n",
        "    # 计算平均推理速度 (ms/sample)\n",
        "    avg_inference_time = (total_time / total_samples) * 1000\n",
        "\n",
        "    # 获取最终评估结果\n",
        "    loss = loss_avg.result().numpy()\n",
        "    top1 = top1_accuracy.result().numpy()\n",
        "    top5 = top5_accuracy.result().numpy()\n",
        "\n",
        "    # 打印评估结果\n",
        "    print(f\"Loss: {loss:.4f}\")\n",
        "    print(f\"Top-1 Accuracy: {top1:.4f}\")\n",
        "    print(f\"Top-5 Accuracy: {top5:.4f}\")\n",
        "    print(f\"Model Size: {model_size_mb:.2f} MB\")\n",
        "    print(f\"Average Inference Time: {avg_inference_time:.2f} ms/sample\")\n",
        "\n",
        "    return top1, top5, model_size_mb, avg_inference_time\n",
        "\n",
        "def evaluate_tflite_model(tflite_model, dataset):\n",
        "    \"\"\"评估TFLite模型的性能\n",
        "    \n",
        "    Args:\n",
        "        tflite_model: TFLite模型的二进制内容\n",
        "        dataset: tf.data.Dataset数据集对象\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (top1准确率, top5准确率, 模型大小, 推理速度)\n",
        "    \"\"\"\n",
        "    # 计算TFLite模型大小 (MB)\n",
        "    model_size_mb = len(tflite_model) / (1024 * 1024)\n",
        "\n",
        "    # 创建并初始化TFLite解释器\n",
        "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # 获取模型输入输出张量的详细信息\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    # 创建评估指标\n",
        "    top1_accuracy = tf.keras.metrics.Accuracy()  # Top-1准确率评估器\n",
        "    top5_accuracy = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)  # Top-5准确率评估器\n",
        "\n",
        "    total_samples = 0  # 样本计数器\n",
        "    total_time = 0  # 总推理时间\n",
        "\n",
        "    # 遍历数据集进行评估\n",
        "    for images, labels in dataset:\n",
        "        # 将one-hot标签转换为类别索引\n",
        "        true_labels = tf.argmax(labels, axis=1).numpy()\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "        # 存储批次预测结果的数组\n",
        "        batch_predictions = np.zeros((batch_size, len(class_names)), dtype=np.float32)\n",
        "\n",
        "        # 对批次中的每个样本进行推理\n",
        "        for i in range(batch_size):\n",
        "            total_samples += 1\n",
        "\n",
        "            # 预处理输入数据\n",
        "            input_data = np.expand_dims(images[i].numpy(), axis=0).astype(np.float32)\n",
        "            \n",
        "            # 记录推理时间\n",
        "            start_time = time.time()\n",
        "            \n",
        "            # 设置输入数据并运行推理\n",
        "            interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "            interpreter.invoke()\n",
        "\n",
        "            # 获取预测结果\n",
        "            batch_predictions[i] = interpreter.get_tensor(output_details[0]['index'])[0]\n",
        "            \n",
        "            end_time = time.time()\n",
        "            total_time += end_time - start_time\n",
        "\n",
        "        # 更新评估指标\n",
        "        top1_accuracy.update_state(\n",
        "            true_labels,\n",
        "            np.argmax(batch_predictions, axis=1)\n",
        "        )\n",
        "        top5_accuracy.update_state(\n",
        "            true_labels,\n",
        "            batch_predictions\n",
        "        )\n",
        "\n",
        "    # 计算平均推理速度 (ms/sample)\n",
        "    avg_inference_time = (total_time / total_samples) * 1000\n",
        "\n",
        "    # 获取最终评估结果\n",
        "    top1 = top1_accuracy.result().numpy()\n",
        "    top5 = top5_accuracy.result().numpy()\n",
        "\n",
        "    # 打印评估结果\n",
        "    print(f\"TFLite模型 Top-1 准确率: {top1:.4f}\")\n",
        "    print(f\"TFLite模型 Top-5 准确率: {top5:.4f}\")\n",
        "    print(f\"TFLite模型大小: {model_size_mb:.2f} MB\")\n",
        "    print(f\"TFLite模型平均推理时间: {avg_inference_time:.2f} ms/sample\")\n",
        "\n",
        "    return top1, top5, model_size_mb, avg_inference_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWZyIg8ET-S2"
      },
      "source": [
        "# MobileNetV1模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VwMV06p-UBQC"
      },
      "outputs": [],
      "source": [
        "def get_MobileNetV1():\n",
        "  \"\"\"\n",
        "  构建并返回一个基于MobileNetV1的迁移学习模型\n",
        "  \n",
        "  模型架构:\n",
        "  1. 使用预训练的MobileNetV1作为特征提取器\n",
        "  2. 添加数据增强层进行训练时的数据增强\n",
        "  3. 添加预处理层对输入进行标准化\n",
        "  4. 添加自定义分类器头部进行具体任务的分类\n",
        "  \n",
        "  返回:\n",
        "  - 构建好的MobileNetV1模型\n",
        "  \"\"\"\n",
        "  # 初始化基础模型 - 使用ImageNet预训练权重,不包含顶层分类器\n",
        "  pre_trained_model = tf.keras.applications.MobileNet(\n",
        "      input_shape=(224, 224, 3),  # 输入图像尺寸为224x224,3通道\n",
        "      include_top=False,  # 不包含顶层分类器\n",
        "      weights='imagenet'  # 加载ImageNet预训练权重\n",
        "  )\n",
        "\n",
        "  # 冻结基础模型的权重 - 固定特征提取器,只训练新添加的分类层\n",
        "  for layer in pre_trained_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  # 应用数据增强和预处理\n",
        "  x = data_augmentation(pre_trained_model.input)  # 添加数据增强提高模型泛化能力\n",
        "  x = tf.keras.applications.mobilenet.preprocess_input(x)  # MobileNet专用的预处理方法\n",
        "\n",
        "  # 添加自定义顶层分类器\n",
        "  x = pre_trained_model(x)  # 通过预训练模型提取特征\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)  # 全局平均池化减少参数量\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)  # dropout层防止过拟合\n",
        "  x = tf.keras.layers.Dense(128, activation='relu')(x)  # 全连接层\n",
        "  x = tf.keras.layers.Dense(256, activation='relu')(x)  # 全连接层增加非线性\n",
        "  predictions = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)  # 输出层,使用softmax进行多分类\n",
        "\n",
        "  # 构建最终模型 - 将输入和输出组装成完整的模型\n",
        "  MobileNetV1 = tf.keras.models.Model(inputs=pre_trained_model.input, outputs=predictions)\n",
        "\n",
        "  return MobileNetV1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVOJo9-FbdpX"
      },
      "source": [
        "# MobileNetV2模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jUdyuZwGbiuk"
      },
      "outputs": [],
      "source": [
        "def get_MobileNetV2():\n",
        "  \"\"\"\n",
        "  构建并返回一个基于MobileNetV2的迁移学习模型\n",
        "  \n",
        "  模型架构:\n",
        "  1. 使用预训练的MobileNetV2作为特征提取器\n",
        "  2. 添加数据增强层进行训练时的数据增强\n",
        "  3. 添加预处理层对输入进行标准化\n",
        "  4. 添加自定义分类器头部进行具体任务的分类\n",
        "  \n",
        "  返回:\n",
        "  - 构建好的MobileNetV2模型\n",
        "  \"\"\"\n",
        "  # 初始化基础模型 - 使用ImageNet预训练权重,不包含顶层分类器\n",
        "  pre_trained_model = tf.keras.applications.MobileNetV2(\n",
        "      input_shape=(224, 224, 3),  # 输入图像尺寸为224x224,3通道\n",
        "      include_top=False,  # 不包含顶层分类器\n",
        "      weights='imagenet'  # 加载ImageNet预训练权重\n",
        "  )\n",
        "\n",
        "  # 冻结基础模型的权重 - 固定特征提取器,只训练新添加的分类层\n",
        "  for layer in pre_trained_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  # 应用数据增强和预处理\n",
        "  x = data_augmentation(pre_trained_model.input)  # 添加数据增强提高模型泛化能力\n",
        "  x = tf.keras.applications.mobilenet_v2.preprocess_input(x)  # MobileNetV2专用的预处理方法\n",
        "\n",
        "  # 添加自定义顶层分类器\n",
        "  x = pre_trained_model(x)  # 通过预训练模型提取特征\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)  # 全局平均池化减少参数量\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)  # dropout层防止过拟合\n",
        "  x = tf.keras.layers.Dense(128, activation='relu')(x)  # 全连接层\n",
        "  x = tf.keras.layers.Dense(256, activation='relu')(x)  # 全连接层增加非线性\n",
        "  predictions = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)  # 输出层,使用softmax进行多分类\n",
        "\n",
        "  # 构建最终模型 - 将输入和输出组装成完整的模型\n",
        "  MobileNetV2 = tf.keras.models.Model(inputs=pre_trained_model.input, outputs=predictions)\n",
        "\n",
        "  return MobileNetV2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "savp0b1IWtC2"
      },
      "source": [
        "# EfficientNet模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "eHvAadCCWyUl"
      },
      "outputs": [],
      "source": [
        "def get_EfficientNetB0():\n",
        "  \"\"\"\n",
        "  构建并返回一个基于EfficientNetB0的迁移学习模型\n",
        "  \n",
        "  模型架构:\n",
        "  1. 使用预训练的EfficientNetB0作为特征提取器,该模型在ImageNet数据集上预训练\n",
        "  2. 添加数据增强层提高模型泛化能力\n",
        "  3. 添加预处理层对输入进行标准化\n",
        "  4. 添加自定义分类器头部进行具体任务的分类\n",
        "  \n",
        "  返回:\n",
        "  - 构建好的EfficientNetB0模型\n",
        "  \"\"\"\n",
        "  # 初始化基础模型 - 使用ImageNet预训练权重,不包含顶层分类器\n",
        "  pre_trained_model = tf.keras.applications.EfficientNetB0(\n",
        "      input_shape=(224, 224, 3),  # 输入图像尺寸为224x224,3通道\n",
        "      include_top=False,  # 不包含顶层分类器\n",
        "      weights='imagenet'  # 加载ImageNet预训练权重\n",
        "  )\n",
        "\n",
        "  # 冻结基础模型的权重 - 固定特征提取器,只训练新添加的分类层\n",
        "  for layer in pre_trained_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  # 应用数据增强和预处理\n",
        "  x = data_augmentation(pre_trained_model.input)  # 添加数据增强提高模型泛化能力 \n",
        "  x = tf.keras.applications.efficientnet.preprocess_input(x)  # EfficientNet专用的预处理方法\n",
        "\n",
        "  # 添加自定义顶层分类器\n",
        "  x = pre_trained_model(x)  # 通过预训练模型提取特征\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)  # 全局平均池化减少参数量\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)  # dropout层防止过拟合\n",
        "  x = tf.keras.layers.Dense(128, activation='relu')(x)  # 全连接层\n",
        "  x = tf.keras.layers.Dense(256, activation='relu')(x)  # 全连接层增加非线性\n",
        "  predictions = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)  # 输出层,使用softmax进行多分类\n",
        "\n",
        "  # 构建最终模型 - 将输入和输出组装成完整的模型\n",
        "  EfficientNetB0 = tf.keras.models.Model(inputs=pre_trained_model.input, outputs=predictions)\n",
        "\n",
        "  return EfficientNetB0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fvu3Ux7k-xq"
      },
      "source": [
        "# ResNet模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CI6EnoEZlG7p"
      },
      "outputs": [],
      "source": [
        "def get_ResNet101():\n",
        "  \"\"\"\n",
        "  构建并返回一个基于ResNet101的迁移学习模型\n",
        "  \n",
        "  模型架构:\n",
        "  1. 使用预训练的ResNet101作为特征提取器,该模型在ImageNet数据集上预训练\n",
        "  2. 添加数据增强层提高模型泛化能力\n",
        "  3. 添加预处理层对输入进行标准化\n",
        "  4. 添加自定义分类器头部进行具体任务的分类\n",
        "  \n",
        "  返回:\n",
        "  - 构建好的ResNet101模型\n",
        "  \"\"\"\n",
        "  # 初始化基础模型 - 使用ImageNet预训练权重,不包含顶层分类器\n",
        "  pre_trained_model = tf.keras.applications.ResNet101(\n",
        "      input_shape=(224, 224, 3),  # 输入图像尺寸为224x224,3通道\n",
        "      include_top=False,  # 不包含顶层分类器\n",
        "      weights='imagenet'  # 加载ImageNet预训练权重\n",
        "  )\n",
        "\n",
        "  # 冻结基础模型的权重 - 固定特征提取器,只训练新添加的分类层\n",
        "  for layer in pre_trained_model.layers:\n",
        "      layer.trainable = False\n",
        "\n",
        "  # 应用数据增强和预处理\n",
        "  x = data_augmentation(pre_trained_model.input)  # 添加数据增强提高模型泛化能力\n",
        "  x = tf.keras.applications.resnet.preprocess_input(x)  # ResNet专用的预处理方法\n",
        "\n",
        "  # 添加自定义顶层分类器\n",
        "  x = pre_trained_model(x)  # 通过预训练模型提取特征\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)  # 全局平均池化减少参数量\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)  # dropout层防止过拟合\n",
        "  x = tf.keras.layers.Dense(128, activation='relu')(x)  # 全连接层\n",
        "  x = tf.keras.layers.Dense(256, activation='relu')(x)  # 全连接层增加非线性\n",
        "  predictions = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)  # 输出层,使用softmax进行多分类\n",
        "\n",
        "  # 构建最终模型 - 将输入和输出组装成完整的模型\n",
        "  ResNet101 = tf.keras.models.Model(inputs=pre_trained_model.input, outputs=predictions)\n",
        "\n",
        "  return ResNet101"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlXKKN2qF1jT"
      },
      "source": [
        "# 训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "tGcDi25UF47l",
        "outputId": "d35026eb-6171-47b6-bf33-62d2157c7348"
      },
      "outputs": [],
      "source": [
        "# 获取MobileNetV1模型\n",
        "MobileNetV1 = get_MobileNetV1()\n",
        "\n",
        "# 训练MobileNetV1模型并保存训练历史\n",
        "history_MobileNetV1 = train_model(MobileNetV1)\n",
        "\n",
        "# 将模型量化并转换为TFLite格式以便部署\n",
        "MobileNetV1_tflite = quantize_and_convert_to_tflite(MobileNetV1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "_vy9vSGUOacN",
        "outputId": "67794e61-df96-4fd2-856d-0158091b2d15"
      },
      "outputs": [],
      "source": [
        "# 获取MobileNetV2模型\n",
        "MobileNetV2 = get_MobileNetV2()\n",
        "\n",
        "# 训练MobileNetV2模型并保存训练历史\n",
        "history_MobileNetV2 = train_model(MobileNetV2)\n",
        "\n",
        "# 将模型量化并转换为TFLite格式以便部署\n",
        "MobileNetV2_tflite = quantize_and_convert_to_tflite(MobileNetV2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "E9GZPspnuU0z",
        "outputId": "7be235aa-61e8-47bf-803e-f70fae4909dd"
      },
      "outputs": [],
      "source": [
        "# 获取EfficientNetB0模型\n",
        "EfficientNetB0 = get_EfficientNetB0()\n",
        "\n",
        "# 训练EfficientNetB0模型并保存训练历史\n",
        "history_EfficientNetB0 = train_model(EfficientNetB0)\n",
        "\n",
        "# 将模型量化并转换为TFLite格式以便部署\n",
        "EfficientNetB0_tflite = quantize_and_convert_to_tflite(EfficientNetB0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "bZopLrSXOcQ7",
        "outputId": "4253acaf-477b-4e91-e58a-9195d25271ff"
      },
      "outputs": [],
      "source": [
        "# 获取ResNet101模型\n",
        "ResNet101 = get_ResNet101()\n",
        "\n",
        "# 训练ResNet101模型并保存训练历史\n",
        "history_ResNet101 = train_model(ResNet101)\n",
        "\n",
        "# 将模型量化并转换为TFLite格式以便部署\n",
        "ResNet101_tflite = quantize_and_convert_to_tflite(ResNet101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 合并所有模型的训练历史数据并可视化比较\n",
        "\n",
        "# 创建一个2x2的子图布局,设置更大的图像尺寸和DPI以提高清晰度\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10), dpi=1000)\n",
        "\n",
        "# 绘制训练准确率对比\n",
        "ax1.plot(history_MobileNetV1.history['accuracy'], label='MobileNetV1', linewidth=2)\n",
        "ax1.plot(history_MobileNetV2.history['accuracy'], label='MobileNetV2', linewidth=2)\n",
        "ax1.plot(history_EfficientNetB0.history['accuracy'], label='EfficientNetB0', linewidth=2)\n",
        "ax1.plot(history_ResNet101.history['accuracy'], label='ResNet101', linewidth=2)\n",
        "ax1.set_title('训练准确率对比', fontsize=12, pad=10)\n",
        "ax1.set_xlabel('轮次', fontsize=10)\n",
        "ax1.set_ylabel('准确率', fontsize=10)\n",
        "ax1.legend(fontsize=9)\n",
        "ax1.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# 绘制验证准确率对比\n",
        "ax2.plot(history_MobileNetV1.history['val_accuracy'], label='MobileNetV1', linewidth=2)\n",
        "ax2.plot(history_MobileNetV2.history['val_accuracy'], label='MobileNetV2', linewidth=2)\n",
        "ax2.plot(history_EfficientNetB0.history['val_accuracy'], label='EfficientNetB0', linewidth=2)\n",
        "ax2.plot(history_ResNet101.history['val_accuracy'], label='ResNet101', linewidth=2)\n",
        "ax2.set_title('验证准确率对比', fontsize=12, pad=10)\n",
        "ax2.set_xlabel('轮次', fontsize=10)\n",
        "ax2.set_ylabel('准确率', fontsize=10)\n",
        "ax2.legend(fontsize=9)\n",
        "ax2.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# 绘制训练损失对比\n",
        "ax3.plot(history_MobileNetV1.history['loss'], label='MobileNetV1', linewidth=2)\n",
        "ax3.plot(history_MobileNetV2.history['loss'], label='MobileNetV2', linewidth=2)\n",
        "ax3.plot(history_EfficientNetB0.history['loss'], label='EfficientNetB0', linewidth=2)\n",
        "ax3.plot(history_ResNet101.history['loss'], label='ResNet101', linewidth=2)\n",
        "ax3.set_title('训练损失对比', fontsize=12, pad=10)\n",
        "ax3.set_xlabel('轮次', fontsize=10)\n",
        "ax3.set_ylabel('损失', fontsize=10)\n",
        "ax3.legend(fontsize=9)\n",
        "ax3.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# 绘制验证损失对比\n",
        "ax4.plot(history_MobileNetV1.history['val_loss'], label='MobileNetV1', linewidth=2)\n",
        "ax4.plot(history_MobileNetV2.history['val_loss'], label='MobileNetV2', linewidth=2)\n",
        "ax4.plot(history_EfficientNetB0.history['val_loss'], label='EfficientNetB0', linewidth=2)\n",
        "ax4.plot(history_ResNet101.history['val_loss'], label='ResNet101', linewidth=2)\n",
        "ax4.set_title('验证损失对比', fontsize=12, pad=10)\n",
        "ax4.set_xlabel('轮次', fontsize=10)\n",
        "ax4.set_ylabel('损失', fontsize=10)\n",
        "ax4.legend(fontsize=9)\n",
        "ax4.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# 调整子图布局\n",
        "plt.tight_layout(pad=3.0)\n",
        "\n",
        "# 保存高清图片\n",
        "plt.savefig('model_comparison.png', dpi=1000, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbUitwGkJ07U"
      },
      "source": [
        "# 评估\n",
        "\n",
        "本节将对训练好的模型进行评估,包括:\n",
        "1. 评估原始模型在验证集上的性能指标(准确率等)\n",
        "2. 评估量化后的TFLite模型在验证集上的性能指标\n",
        "3. 将量化后的TFLite模型保存到文件中以便部署\n",
        "\n",
        "我们将依次评估以下4个模型:\n",
        "- MobileNetV1: 轻量级CNN模型,适合移动端部署\n",
        "- MobileNetV2: MobileNetV1的改进版本,性能更好\n",
        "- EfficientNetB0: 高效CNN模型,在准确率和效率之间取得平衡\n",
        "- ResNet101: 经典的深度残差网络,性能强大但计算量较大"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV1PV-blGVlT",
        "outputId": "a78a3773-4ab8-4597-af8c-117daea41672"
      },
      "outputs": [],
      "source": [
        "# 评估并保存MobileNetV1模型\n",
        "print(\"MobileNetV1:\\n\")\n",
        "# 评估原始模型在验证集上的性能\n",
        "evaluate_model(MobileNetV1, validation_dataset)\n",
        "# 评估量化后的TFLite模型在验证集上的性能\n",
        "evaluate_tflite_model(MobileNetV1_tflite, validation_dataset)\n",
        "# 将量化后的TFLite模型保存到文件\n",
        "save_model(MobileNetV1_tflite, \"MobileNetV1.tflite\")\n",
        "\n",
        "# 评估并保存MobileNetV2模型\n",
        "print(\"\\nMobileNetV2:\\n\") \n",
        "# 评估原始模型在验证集上的性能\n",
        "evaluate_model(MobileNetV2, validation_dataset)\n",
        "# 评估量化后的TFLite模型在验证集上的性能\n",
        "evaluate_tflite_model(MobileNetV2_tflite, validation_dataset)\n",
        "# 将量化后的TFLite模型保存到文件\n",
        "save_model(MobileNetV2_tflite, \"MobileNetV2.tflite\")\n",
        "\n",
        "# 评估并保存EfficientNetB0模型\n",
        "print(\"\\nEfficientNetB0:\\n\")\n",
        "# 评估原始模型在验证集上的性能\n",
        "evaluate_model(EfficientNetB0, validation_dataset)\n",
        "# 评估量化后的TFLite模型在验证集上的性能\n",
        "evaluate_tflite_model(EfficientNetB0_tflite, validation_dataset)\n",
        "# 将量化后的TFLite模型保存到文件\n",
        "save_model(EfficientNetB0_tflite, \"EfficientNetB0.tflite\")\n",
        "\n",
        "# 评估并保存ResNet101模型\n",
        "print(\"\\nResNet101:\\n\")\n",
        "# 评估原始模型在验证集上的性能\n",
        "evaluate_model(ResNet101, validation_dataset)\n",
        "# 评估量化后的TFLite模型在验证集上的性能\n",
        "evaluate_tflite_model(ResNet101_tflite, validation_dataset)\n",
        "# 将量化后的TFLite模型保存到文件\n",
        "save_model(ResNet101_tflite, \"ResNet101.tflite\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 11005152,
          "datasetId": 6602109,
          "sourceId": 10661516,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "py3_11_11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
