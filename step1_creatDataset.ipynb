{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"蘑菇分类数据集创建脚本\n======================\n\n本脚本实现了蘑菇图像数据集的自动化收集过程，包括：\n1. 从百度图片搜索下载蘑菇图像，创建分类数据集\n2. 从百度百科抓取蘑菇描述信息，存储到SQLite数据库\n\n涵盖36种常见食用菌的图像和相关信息。","metadata":{}},{"cell_type":"code","source":"!pip install selenium webdriver_manager\n\nimport os\nimport time\nimport requests\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.chrome.options import Options\nfrom webdriver_manager.chrome import ChromeDriverManager\nimport sqlite3\nfrom PIL import Image\nfrom io import BytesIO\n\n# 添加Google Chrome的存储库\n!wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add -\n!echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google-chrome.list\n\n# 更新包列表\n!apt-get update\n\n# 安装Chrome浏览器\n!apt-get install -y google-chrome-stable\n\n# 检查Chrome是否安装成功\n!google-chrome-stable --version","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 常量定义","metadata":{}},{"cell_type":"code","source":"# 36种常见食用菌名称\nMUSHROOM_TYPES = [\n    \"羊肚菌\", \"牛肝菌\", \"鸡油菌\", \"鸡枞菌\", \"青头菌\", \"奶浆菌\", \"干巴菌\", \"虎掌菌\",\n    \"白葱牛肝菌\", \"老人头菌\", \"猪肚菌\", \"谷熟菌\", \"白参菌\", \"黑木耳\", \"银耳\", \"金耳\",\n    \"猴头菇\", \"香菇\", \"平菇\", \"金针菇\", \"口蘑\", \"鹿茸菇\", \"榆黄蘑\", \"榛蘑\", \"草菇\",\n    \"鸡腿菇\", \"茶树菇\", \"蟹味菇\", \"白玉菇\", \"红菇\", \"杏鲍菇\", \"松茸\", \"姬松茸\", \"松露\",\n    \"竹荪\", \"虫草花\"\n]\n\n# 每种蘑菇需要收集的图像数量\nIMAGES_PER_TYPE = 5\n\n# 数据目录\nDATA_DIR = \"data\"\n\n# 图像存储目录\nIMAGES_DIR = \"images\"\n\n# 数据库文件名\nDATABASE_FILE = 'mushrooms.db'\n\n# 标签文件\nLABEL_FILE = 'label.txt'\n\n# 请求头\nHEADERS = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n                  '(KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36 Edg/132.0.0.0'\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 图像下载相关函数","metadata":{}},{"cell_type":"code","source":"def setup_chrome_driver():\n    \"\"\"\n    设置并初始化Chrome WebDriver\n    \n    返回:\n        webdriver.Chrome: 配置好的Chrome WebDriver实例\n    \"\"\"\n    chrome_options = webdriver.ChromeOptions()\n    chrome_options.add_argument(\"--headless\")\n    chrome_options.add_argument(\"--no-sandbox\")\n    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n    \n    # 在Colab环境中指定Chrome的路径\n    chrome_options.binary_location = '/usr/bin/google-chrome'\n    \n    return webdriver.Chrome(\n        service=Service(ChromeDriverManager().install()),\n        options=chrome_options\n    )\n\ndef download_images(query_i, query, num_images=IMAGES_PER_TYPE, max_retries=3):\n    \"\"\"\n    从百度图片搜索下载指定关键词的图像\n    \n    参数:\n        query_i (int): 查询索引，用于创建目录名\n        query (str): 查询关键词，例如蘑菇名称\n        num_images (int): 要下载的图像数量\n        max_retries (int): 下载失败时的最大重试次数\n        \n    返回:\n        int: 成功下载的图像数量\n    \"\"\"\n    # 初始化WebDriver\n    driver = setup_chrome_driver()\n\n    # 创建存储图片的目录\n    query_dir = os.path.join(DATA_DIR, f\"class{query_i}\")\n    if not os.path.exists(query_dir):\n        os.makedirs(query_dir)\n\n    # 搜索URL\n    search_url = f\"https://image.baidu.com/search/index?tn=baiduimage&word={query}\"\n\n    try:\n        driver.get(search_url)\n        images = set()  # 使用集合避免重复图像\n        \n        # 滚动页面以加载更多图像直到满足需求\n        scroll_attempts = 0\n        max_scroll_attempts = 20  # 防止无限滚动\n        \n        while len(images) < num_images and scroll_attempts < max_scroll_attempts:\n            # 获取所有图片元素\n            soup = BeautifulSoup(driver.page_source, 'html.parser')\n            img_tags = soup.find_all('img', class_='main_img')\n\n            # 收集图像URL\n            for img in img_tags:\n                src = img.get('src')\n                if src and src.startswith('http'):\n                    images.add(src)\n\n            # 向下滚动页面以加载更多图片\n            driver.execute_script(\"window.scrollBy(0, document.body.scrollHeight);\")\n            time.sleep(2)  # 等待页面加载更多图片\n            scroll_attempts += 1\n\n        print(f\"找到 {len(images)} 张 {query} 的图像。\")\n\n        # 将集合转换为列表并限制数量\n        images_list = list(images)[:num_images]\n\n        # 下载图片\n        successful_downloads = 0\n        for i, img_url in enumerate(images_list):\n            retries = 0\n            while retries < max_retries:\n                try:\n                    response = requests.get(img_url, timeout=5)\n                    if response.status_code == 200:\n                        with open(os.path.join(query_dir, f\"{i}.jpg\"), \"wb\") as file:\n                            file.write(response.content)\n                        print(f\"已下载图像 {i+1}/{num_images} - {query}\")\n                        successful_downloads += 1\n                        break  # 下载成功，跳出重试循环\n                    else:\n                        print(f\"图像 {i+1} 下载失败: HTTP状态码 {response.status_code}\")\n                except Exception as e:\n                    print(f\"尝试 {retries+1}/{max_retries}: 下载图像 {i+1} 失败: {e}\")\n                retries += 1\n                time.sleep(1)  # 短暂延迟后重试\n                \n        print(f\"成功下载 {successful_downloads}/{num_images} 张 {query} 的图像\")\n        return successful_downloads\n\n    finally:\n        driver.quit()  # 确保关闭浏览器\n\ndef rename_images_in_folders(data_dir):\n    \"\"\"\n    对指定目录中的所有图像文件按序号重命名\n    \n    参数:\n        data_dir (str): 包含类别子文件夹的数据目录路径\n    \"\"\"\n    # 遍历data目录下的所有子文件夹\n    for folder_name in os.listdir(data_dir):\n        folder_path = os.path.join(data_dir, folder_name)\n        \n        if os.path.isdir(folder_path):\n            print(f\"处理文件夹: {folder_name}\")\n            # 获取文件夹中所有的图片文件\n            image_files = [f for f in os.listdir(folder_path) \n                          if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif'))]\n            \n            print(f\"找到 {len(image_files)} 个图像文件\")\n            \n            # 重命名图片文件\n            success_count = 0\n            for i, old_filename in enumerate(image_files):\n                old_file_path = os.path.join(folder_path, old_filename)\n                new_filename = f\"{i}.jpg\"\n                new_file_path = os.path.join(folder_path, new_filename)\n                \n                # 如果目标文件已存在，先删除\n                if os.path.exists(new_file_path) and old_file_path != new_file_path:\n                    os.remove(new_file_path)\n                \n                try:\n                    # 如果源文件和目标文件不同，则进行重命名\n                    if old_file_path != new_file_path:\n                        os.rename(old_file_path, new_file_path)\n                        success_count += 1\n                except Exception as e:\n                    print(f\"重命名 {old_filename} 失败: {e}\")\n            \n            print(f\"在 {folder_name} 中成功重命名 {success_count} 个文件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 第一步: 下载蘑菇图像数据集","metadata":{}},{"cell_type":"code","source":"def download_all_mushroom_images():\n    \"\"\"\n    下载所有蘑菇类型的图像并创建标签映射\n    \"\"\"\n    # 确保数据目录存在\n    if not os.path.exists(DATA_DIR):\n        os.makedirs(DATA_DIR)\n    \n    # 创建标签映射文件并开始下载图像\n    with open(LABEL_FILE, 'w', encoding='utf-8') as f:\n        for i, keyword in enumerate(MUSHROOM_TYPES):\n            print(f\"\\n开始下载 {keyword} 的图像（类别{i}）...\")\n            download_images(i, keyword, IMAGES_PER_TYPE)\n            f.write(f\"{keyword} class{i}\\n\")  # 添加换行符以提高可读性\n            \n    print(f\"所有图像下载完成。标签映射已保存到{LABEL_FILE}文件。\")\n    \n    # 重命名所有图像文件\n    rename_images_in_folders(DATA_DIR)\n    print(\"图像重命名完成\")\n\ndownload_all_mushroom_images()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 百度百科信息抓取相关函数","metadata":{}},{"cell_type":"code","source":"def fetch_mushroom_info(keyword):\n    \"\"\"\n    从百度百科获取蘑菇信息\n    \n    参数:\n        keyword (str): 蘑菇名称关键词\n        \n    返回:\n        tuple: (描述, 图像URL)\n    \"\"\"\n    url = f\"https://baike.baidu.com/item/{keyword}\"\n    \n    try:\n        response = requests.get(url, headers=HEADERS, allow_redirects=True, timeout=10)\n        response.raise_for_status()  # 抛出HTTP错误状态\n    except requests.exceptions.RequestException as e:\n        print(f\"请求异常: {e}\")\n        return None, None\n    \n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # 获取<meta name=\"description\">标签的内容\n    meta_tag = soup.find('meta', attrs={'name': 'description'})\n    description = meta_tag['content'].strip() if meta_tag and 'content' in meta_tag.attrs else None\n\n    # 获取第一张图片的URL\n    img_tag = soup.find('meta', attrs={'name': 'image'})\n    img_url = img_tag['content'].strip() if img_tag and 'content' in img_tag.attrs else None\n\n    return description, img_url\n\ndef download_image(name, img_url, folder=IMAGES_DIR):\n    \"\"\"\n    下载并保存图像\n    \n    参数:\n        name (str): 图像名称\n        img_url (str): 图像URL\n        folder (str): 保存文件夹\n        \n    返回:\n        str: 保存的图像路径\n    \"\"\"\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n\n    try:\n        response = requests.get(img_url, timeout=10)\n        response.raise_for_status()\n        \n        img_name = os.path.join(folder, f\"{name}.jpg\")\n        with open(img_name, 'wb') as handler:\n            handler.write(response.content)\n        return img_name\n    except Exception as e:\n        print(f\"下载图像失败: {e}\")\n        return None\n\ndef create_database():\n    \"\"\"\n    创建SQLite数据库和表结构\n    \"\"\"\n    with sqlite3.connect(DATABASE_FILE) as conn:\n        c = conn.cursor()\n        c.execute('''CREATE TABLE IF NOT EXISTS mushrooms\n                    (name TEXT PRIMARY KEY, description TEXT, image_path TEXT)''')\n        conn.commit()\n\ndef save_to_database(name, description, image_path):\n    \"\"\"\n    将蘑菇信息保存到数据库\n    \n    参数:\n        name (str): 蘑菇名称\n        description (str): 蘑菇描述\n        image_path (str): 图像文件路径\n    \"\"\"\n    with sqlite3.connect(DATABASE_FILE) as conn:\n        c = conn.cursor()\n        # 检查是否已存在相同名称的记录\n        c.execute(\"SELECT * FROM mushrooms WHERE name = ?\", (name,))\n        if c.fetchone():\n            # 更新现有记录\n            c.execute(\"UPDATE mushrooms SET description = ?, image_path = ? WHERE name = ?\", \n                     (description, image_path, name))\n        else:\n            # 插入新记录\n            c.execute(\"INSERT INTO mushrooms (name, description, image_path) VALUES (?, ?, ?)\", \n                     (name, description, image_path))\n        conn.commit()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 第二步: 获取蘑菇描述信息","metadata":{}},{"cell_type":"code","source":"def fetch_and_save_mushroom_info():\n    \"\"\"\n    获取并保存所有蘑菇信息到数据库\n    \"\"\"\n    create_database()\n    success_count = 0\n    \n    for keyword in MUSHROOM_TYPES:\n        print(f\"获取 {keyword} 的信息...\")\n        description, img_url = fetch_mushroom_info(keyword)\n        image_path = None\n        \n        if img_url:\n            try:\n                image_path = download_image(keyword, img_url)\n                print(f\"已下载 {keyword} 的图像: {image_path}\")\n            except Exception as e:\n                print(f\"下载 {keyword} 的图像失败: {e}\")\n\n        if description:\n            save_to_database(keyword, description, image_path)\n            print(f\"已保存 {keyword} 的信息。\")\n            print(description)\n            success_count += 1\n        else:\n            print(f\"未找到 {keyword} 的信息。\")\n    \n    print(f\"成功获取了 {success_count}/{len(MUSHROOM_TYPES)} 种蘑菇的信息\")\n\nfetch_and_save_mushroom_info()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}